Automatically generated by Mendeley Desktop 1.17-dev1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Efron2010,
abstract = {Common measures of term importance in information retrieval (IR) rely on counts of term frequency; rare terms receive higher weight in document ranking than common terms receive. However, realistic scenarios yield additional information about terms in a collection. Of interest in this paper is the temporal behavior of terms as a collection changes over time. We propose capturing each term's collection frequency at discrete time intervals over the lifespan of a corpus and analyzing the resulting time series. We hypothesize the collection frequency of a term x at time t is predictable by a linear model of the term's prior observations. On the other hand, a linear time series model for a strong discriminators' collection frequency will yield a poor fit to the data. Operationalizing this hypothesis, we induce three time-based measures of term importance and test these against state-of-the-art term weighting models.},
author = {Efron, Miles},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Efron - 2010 - Linear Time Series Models for Term Weighting in Information Retrieval.pdf:pdf},
journal = {JASIST},
number = {7},
pages = {1299--1312},
title = {{Linear time series models for term weighting in information retrieval}},
volume = {61},
year = {2010}
}
@book{Macdonald2013,
abstract = {Web search engines are increasingly deploying many features, combined using learning to rank techniques. However, various practical questions remain concerning the manner in which learning to rank should be deployed. For instance, a sample of documents with sufficient recall is used, such that re-ranking of the sample by the learned model brings the relevant documents to the top. However, the properties of the document sample such as when to stop ranking—i.e. its minimum effective size—remain unstudied. Similarly, effective listwise learning to rank techniques minimise a loss function corresponding to a standard information retrieval evaluation measure. However, the appropriate choice of how to calculate the loss function—i.e. the choice of the learning evaluation measure and the rank depth at which this measure should be calculated—are as yet unclear. In this paper, we address all of these issues by formulating various hypotheses and research questions, before performing exhaustive experiments using multiple learning to rank techniques and different types of information needs on the ClueWeb09 and LETOR corpora. Among many conclusions, we find, for instance, that the smallest effective sample for a given query set is dependent on the type of information need of the queries, the document representation used during sampling and the test evaluation measure. As the sample size is varied, the selected features markedly change—for instance, we find that the link analysis features are favoured for smaller document samples. Moreover, despite reflecting a more realistic user model, the recently proposed ERR measure is not as effective as the traditional NDCG as a learning loss function. Overall, our comprehensive experiments provide the first empirical derivation of best practices for learning to rank deployments.},
author = {Macdonald, Craig and Santos, Rodrygo L T and Ounis, Iadh},
booktitle = {Information Retrieval},
doi = {10.1007/s10791-012-9209-9},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Macdonald, Santos, Ounis - 2013 - The whens and hows of learning to rank for web search.pdf:pdf},
isbn = {1079101292},
issn = {13864564},
keywords = {Document representations,Evaluation,Learning to rank,Loss function,Sample size,Web search},
number = {5},
pages = {584--628},
title = {{The whens and hows of learning to rank for web search}},
volume = {16},
year = {2013}
}
@article{Belkin1982,
abstract = {Discusses background and theory underlying a design study for an interactive information retrieval system funded by the British Library Research and Development Department which will determine structural representations of anomalous states of knowledge (ASKs) underlying information needs. References are cited. (EJS)},
author = {Oddy, Robert N and Belkin, Nicholas J and Brooks, Helen M},
doi = {10.1108/eb026722},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Belkin, Oddy, Brooks - 1982 - Ask for Information Retrieval Part I. Background and Theory.pdf:pdf},
isbn = {1558604545},
issn = {0022-0418},
journal = {Journal of Documentation},
number = {2},
pages = {61--71},
title = {{Ask for information retrieval: Part I. background and theory}},
url = {http://dx.doi.org/10.1108/eb026722},
volume = {38},
year = {1982}
}
@article{Swanson1988,
abstract = {The article presents the historical aspects of information retrieval (IR). Some experimental tests of information systems have yielded good retrieval results and some very poor results. Automatic keyword-in-context indexing was not yet born, though its conceptual origin dates back at least to the year 1247. The decade of the 1950s was marked by endless disputes among proponents of various breeds of indexing schemes, classification systems, and bird-dogs. The Blair-Maron experiment, based on a collection of some 400,000 documents, offers valuable insight into the nature and the subtlety of the conceptual problems of IR-the problems of meaning. An important unintended consequence of specialization is the failure of the various branches of science to fertilize one another.},
author = {Swanson, Don R},
doi = {10.1002/(SICI)1097-4571(198803)39:2<92::AID-ASI4>3.0.CO;2-P},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Swanson - 1988 - Historical note Information retrieval and the future of an illusion.pdf:pdf},
isbn = {1097-4571},
issn = {00028231},
journal = {JASIS},
keywords = {IR},
number = {2},
pages = {92--98},
title = {{Historical note: Information retrieval and the future of an illusion}},
url = {http://doi.wiley.com/10.1002/(SICI)1097-4571(198803)39:2{\%}3C92::AID-ASI4{\%}3E3.0.CO;2-P},
volume = {39},
year = {1988}
}
@article{Zhai2006,
author = {Zhai, ChengXiang and Lafferty, John D},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/A Risk Minimization Framework for Information Retrieval.pdf:pdf},
journal = {Inf. Process. Manage.},
number = {1},
pages = {31--55},
title = {{A risk minimization framework for information retrieval}},
volume = {42},
year = {2006}
}
@inproceedings{Liu2004,
abstract = {Previous research on cluster-based retrieval has been inconclusive as to whether it does bring improved retrieval effectiveness over document-based retrieval. Recent developments in the language modeling approach to IR have motivated us to re-examine this problem within this new retrieval framework. We propose two new models for cluster-based retrieval and evaluate them on several TREC collections. We show that cluster-based retrieval can perform consistently across collections of realistic size, and significant improvements over document-based retrieval can be obtained in a fully automatic manner and without relevance information provided by human.},
author = {Liu, Xiaoyong and Croft, W Bruce},
booktitle = {SIGIR '04},
doi = {10.1145/1008992.1009026},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Liu, Croft - 2004 - Cluster-based retrieval using language models.pdf:pdf},
isbn = {1581138814},
keywords = {Cluster Model,Cluster-based Retrieval,Clusterbased,Clustering,Hierarchical Clustering,Information Retrieval,Language Model,Query-specific,Smoothing,Static Clustering,Topic Model},
pages = {186--193},
title = {{Cluster-based retrieval using language models}},
url = {http://portal.acm.org/citation.cfm?doid=1008992.1009026},
year = {2004}
}
@inproceedings{Ji2011,
abstract = {In this paper we give an overview of the Knowledge Base Population (KBP) track at the 2010 Text Analysis Conference. The main goal of KBP is to promote research in discovering facts about entities and augmenting a knowledge base (KB) with these facts. This is done through two tasks, Entity Linking -- linking names in context to entities in the KB -- and Slot Filling -- adding information about an entity to the KB. A large source collection of newswire and web documents is provided from which systems are to discover information. Attributes ("slots") derived from Wikipedia infoboxes are used to create the reference KB. In this paper we provide an overview of the techniques which can serve as a basis for a good KBP system, lay out the remaining challenges by comparison with traditional Information Extraction (IE) and Question Answering (QA) tasks, and provide some suggestions to address these challenges.},
author = {Ji, Heng and Grishman, Ralph},
booktitle = {ACL '11},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Ji, Grishman - 2011 - Knowledge Base Population Successful Approaches and Challenges.pdf:pdf},
isbn = {978-1-932432-87-9},
pages = {1148--1158},
title = {{Knowledge base population: successful approaches and challenges}},
year = {2011}
}
@inproceedings{Karimzadehgan2010,
abstract = {As a principled approach to capturing semantic relations of words in information retrieval, statistical translation models have been shown to outperform simple document language models which rely on exact matching of words in the query and documents. A main challenge in applying translation models to ad hoc information retrieval is to estimate a translation model without training data. Existing work has relied on training on synthetic queries generated based on a document collection. However, this method is computationally expensive and does not have a good coverage of query words. In this paper, we propose an alternative way to estimate a translation model based on normalized mutual information between words, which is less computationally expensive and has better coverage of query words than the synthetic query method of estimation. We also propose to regularize estimated translation probabilities to ensure sufficient probability mass for self-translation. Experiment results show that the proposed mutual information-based estimation method is not only more efficient, but also more effective than the synthetic query-based method, and it can be combined with pseudo-relevance feedback to further improve retrieval accuracy. The results also show that the proposed regularization strategy is effective and can improve retrieval accuracy for both synthetic query-based estimation and mutual information-based estimation.},
author = {Karimzadehgan, Maryam and Zhai, ChengXiang},
booktitle = {SIGIR '10},
doi = {10.1145/1835449.1835505},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Karimzadehgan, Zhai - 2010 - Estimation of Statistical Translation Models Based on Mutual Information for Ad Hoc Information Retrieval.pdf:pdf},
isbn = {978-1-4503-0153-4},
keywords = {all or part of,estima-,feedback,language models,or hard copies of,permission to make digital,smoothing,statistical machine translation,this work for,tion},
pages = {323--330},
title = {{Estimation of statistical translation models based on mutual information for ad hoc information retrieval}},
year = {2010}
}
@misc{Granovetter1973,
abstract = {Analysis of social networks is suggested as a tool for linking micro and macro levels of sociological theory. The procedure is illustrated by elaboration of the macro implications of one aspect of small-scale interaction: the strength of dyadic ties. It is argued that the degree of overlap of two individuals' friendship networks varies directly with the strength of their tie to one another. The impact of this principle on diffusion of influence and information, mobility opportunity, and community organization is explored. Stress is laid on the cohesive power of weak ties. Most network models deal, implicitly, with strong ties, thus confining their applicability to small, well-defined groups. Emphasis on weak ties lends itself to discussion of relations between groups and to analysis of segments of social structure not easily defined in terms of primary groups.},
author = {Granovetter, Mark S.},
booktitle = {American Journal of Sociology},
doi = {10.1086/225469},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Granovetter - 1973 - The Strength of Weak Ties.pdf:pdf},
isbn = {9783540723462},
issn = {0002-9602},
number = {6},
pages = {1360},
pmid = {19848562},
title = {{The strength of weak ties}},
volume = {78},
year = {1973}
}
@phdthesis{Metzler2007,
abstract = {Current state of the art information retrieval models treat documents and queries as bags of words. There have been many attempts to go beyond this simple few have shown consistent improvements in retrieval effectiveness across a wide range of tasks and data sets. Here, we propose a new statistical model for information retrieval based on Markov random fields. The proposed model goes beyond the bag of words assumption by allowing dependencies between terms to be incorporated into the model. This allows for a variety of textual and nontextual features to be easily combined under the umbrella of a single model. Within this framework, we explore the theoretical issues involved, parameter estimation, feature selection, and query expansion. We give experimental results from a number of information retrieval tasks, such as ad hoc retrieval and web search.},
author = {Metzler, Donald},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Metzler - 2007 - Beyond Bags of Words Effectively Modeling Dependence and Features in Information Retrieval.pdf:pdf},
issn = {01635840},
pages = {6--6},
school = {University of Massachusetts Amherst},
title = {{Beyond bags of words : Effectively modeling dependence and features in information retrieval}},
url = {http://ciir.cs.umass.edu/{~}metzler/metzler-thesis.pdf},
year = {2007}
}
@article{Maron2008,
abstract = {The motivation behind "Probabilistic Indexing" was to replace two-valued thinking about information retrieval with probabilistic notions. This involved a new view of the information retrieval problem - viewing it as problem of inference and prediction, and introducing probabilistically weighted indexes and probabilistically ranked output. These ideas were first formulated and written up in August 1958. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Maron, M. E.},
doi = {10.1016/j.ipm.2007.02.012},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Maron - 2008 - An Historical Note on the Origins of Probabilistic Indexing.pdf:pdf},
issn = {03064573},
journal = {Information Processing and Management},
keywords = {Probabilistic information retrieval},
number = {2},
pages = {971--972},
title = {{An historical note on the origins of probabilistic indexing}},
volume = {44},
year = {2008}
}
@book{Casella2006,
abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a {\~{}}without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
author = {Cowles, Mary Kathryn},
booktitle = {Springer Texts in Statistics},
doi = {10.1016/j.peva.2007.06.006},
editor = {Casella, G and Fienberg, SE and Olkin, I},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Casella, Fienberg, Olkin - 2006 - Springer Texts in Statistics.pdf:pdf},
isbn = {9780387781884},
issn = {01621459},
pages = {618},
pmid = {10911016},
title = {{Applied Bayesian Statistics}},
url = {http://books.google.com/books?id=9tv0taI8l6YC},
volume = {102},
year = {2006}
}
@article{Hubert2009,
annote = {The specifics are different, but I see the similarity between this and my entity work. Specifically, they use the document as a query and match it to a category based on the term overlap between the document and the category descriptor.

Category descriptions are sparse and they find they have to enrich them. 

I don't see how this paper was published. Their contribution is meant to be the creation of a retrieval function that works for both ad-hoc and category retrieval. But for category retrieval, they basically just say "reuse the ad-hoc function" (which makes no sense, because who assigns a category to a document more than one time?).},
author = {Hubert, Gilles and Mothe, Josiane},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/21091{\_}ftp.pdf:pdf},
journal = {JASIS},
number = {8},
pages = {1625--1634},
title = {{An adaptable search engine for multimodal information retrieval}},
volume = {60},
year = {2009}
}
@article{Kurland2009,
author = {Kurland, Oren},
doi = {10.1007/s10791-008-9065-9},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/423685c994ac2e9cc01da488fae76fc2fdf5.pdf:pdf},
issn = {13864564},
journal = {Information Retrieval},
keywords = {Cluster-based language models,Cluster-based re-ranking,Cluster-based smoothing,Query-specific clusters},
number = {4},
pages = {437--460},
title = {{Re-ranking search results using language models of query-specific clusters}},
volume = {12},
year = {2009}
}
@inproceedings{Chang2009,
author = {Chang, Jonathan and Boyd-Graber, Jordan and Gerrish, Sean and Wang, Chong and Blei, David M.},
booktitle = {Advances in Neural Information Processing Systems},
doi = {10.1.1.100.1089},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Chang et al. - 2009 - Reading tea leaves How humans interpret topic models.pdf:pdf},
isbn = {9781615679119},
pages = {288--296},
title = {{Reading tea leaves: How humans interpret topic models}},
year = {2009}
}
@article{Hutchins1978,
abstract = {The common view of the aboutness of documents is that the index entries (or classifications) assigned to documents represent or indicate in some way the total contents of documents; indexing and classifying are seen as processes involving the summarization of the texts of documents. In this paper an alternative concept of aboutness is proposed based on an analysis of the linguistic organization of texts, which is felt to be more appropriate in many indexing environments (particularly in non-specialized libraries and information services) and which has implications for the evaluation of the effectiveness of indexing systems.},
author = {Hutchins, W.J.},
doi = {10.1108/eb050629},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Hutchins - 1978 - The concept of ‘aboutness' in subject indexing.pdf:pdf},
isbn = {0001-253X},
issn = {0001-253X},
journal = {Aslib Proceedings},
keywords = {Aslib Proceedings 30 (5) 1978},
number = {5},
pages = {172--181},
pmid = {189},
title = {{The concept of ‘aboutness' in subject indexing}},
volume = {30},
year = {1978}
}
@article{Saracevic2007a,
author = {Saracevic, Tefko},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Saracevic - 2007 - Relevance A Review of the Literature and a Framework for Thinking on the Notion in Information Science. Part II Natur.pdf:pdf},
isbn = {9783848215430},
issn = {14923831},
journal = {JASIST},
number = {13},
pages = {1915--1933},
title = {{Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: nature and manifestations of relevance}},
volume = {58},
year = {2007}
}
@inproceedings{Bennett2013,
author = {Bennett, Paul. N. and Gabrilovich, Evgeniy and Kamps, Jaap and Karlgren, Jussi},
booktitle = {CIKM '13},
doi = {10.1145/2505515.2505808},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Bennett et al. - 2013 - Sixth workshop on exploiting semantic annotations in information retrieval (ESAIR'13).pdf:pdf},
isbn = {9781450322638},
keywords = {knowledge resources,non-topicality,semantic annotation},
pages = {2543--2544},
title = {{Sixth workshop on exploiting semantic annotations in information retrieval (ESAIR'13)}},
url = {http://dl.acm.org/citation.cfm?id=2505515.2505808},
year = {2013}
}
@article{Robertson1976,
annote = {This is for feedback! Relevance information only comes from feedback.
- See SLMIR for confirmation (citation [56])

Binary Independence Model
- "binary index descriptions" (130) means presence/absence of terms in documents

Can be used retrospectively to determine optimal performance, or "predictively" in interactive retrieval (i.e. with feedback)},
author = {Robertson, Stephen and {Sp{\"{a}}rck Jones}, Karen},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Robertson, Sp{\"{a}}rck Jones - 1976 - Relevance Weighting of Search Terms.pdf:pdf},
journal = {JASIS},
number = {3},
pages = {129--146},
title = {{Relevance weighting of search terms}},
volume = {27},
year = {1976}
}
@inproceedings{Weerkamp2009,
author = {Weerkamp, Wouter and Balog, K and de Rijke, Maarten},
booktitle = {ACL '09},
doi = {10.3115/1690219.1690294},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Weerkamp, Balog, Rijke - 2009 - A generative blog post retrieval model that uses query expansion based on external collections.pdf:pdf},
isbn = {9781932432466},
pages = {1057--1065},
title = {{A generative blog post retrieval model that uses query expansion based on external collections}},
url = {http://dl.acm.org/citation.cfm?id=1690294},
year = {2009}
}
@inproceedings{Wei2006,
abstract = {Search algorithms incorporating some form of topic model have a long history in information retrieval. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation (LDA), is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use LDA to improve ad-hoc retrieval. We propose an LDA-based document model within the language modeling framework, and evaluate it on several TREC collections. Gibbs sampling is employed to conduct approximate inference in LDA and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency.},
author = {Wei, Xing and Croft, W Bruce},
booktitle = {SIGIR '06},
doi = {10.1145/1148170.1148204},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Wei, Croft - 2006 - LDA-based document models for ad-hoc retrieval.pdf:pdf},
isbn = {1595933697},
issn = {00295515},
keywords = {allocation,document model,information retrieval,language model,latent,lda,topic model},
pages = {178--185},
title = {{LDA-based document models for ad-hoc retrieval}},
year = {2006}
}
@inproceedings{Zhai2001,
author = {Zhai, ChengXiang and Lafferty, John D},
booktitle = {CIKM '01},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Zhai, Lafferty - 2001 - Model-based Feedback in the Language Modeling Approach to Information Retrieval.pdf:pdf},
isbn = {1-58113-436-3},
pages = {403--410},
title = {{Model-based feedback in the language modeling approach to information retrieval}},
year = {2001}
}
@article{Sherriff1999,
author = {Sherriff, C.},
doi = {10.1136/ip.5.3.235},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Sherriff - 1999 - Inequalities in health.pdf:pdf},
isbn = {0028-4793 (Print)0028-4793 (Linking)},
issn = {1353-8047},
journal = {Injury Prevention},
number = {3},
pages = {235--236},
pmid = {10518274},
title = {{Inequalities in health}},
volume = {5},
year = {1999}
}
@book{Croft2010,
author = {Croft, W Bruce and Metzler, Donald and Strohmann, Trevor},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley//Croft, Metzler, Strohmann - 2010 - Search Engines Information Retrieval in Practice.pdf:pdf},
publisher = {Addison-Wesley Reading},
title = {{Search engines: Information retrieval in practice}},
year = {2010}
}
@inproceedings{Dalton2014,
abstract = {Recent advances in automatic entity linking and knowledge base construction have resulted in entity annotations for document and query collections. For example, annotations of entities from large general purpose knowledge bases, such as Freebase and the Google Knowledge Graph. Understanding how to leverage these entity annotations of text to improve ad hoc document retrieval is an open research area. Query expansion is a commonly used technique to improve retrieval effectiveness. Most previous query expansion approaches focus on text, mainly using unigram concepts. In this paper, we propose a new technique, called entity query feature expansion (EQFE) which enriches the query with features from entities and their links to knowledge bases, including structured attributes and text. We experiment using both explicit query entity annotations and latent entities. We evaluate our technique on TREC text collections automatically annotated with knowledge base entity links, including the Google Freebase Annotations (FACC1) data. We find that entity-based feature expansion results in significant improvements in retrieval effectiveness over state-of-the-art text expansion approaches.},
annote = {Importance:

Uses a variety of "vocabularies," especially including words and "neighboring" entities for entity mentions. See Figure 2.

Uses features and an LTR method to weight these features. Fundamentally a machine learning task.

Finds that ClueWeb09 doesn't work well because of a) poorly annotated entities and b) too many non-relevant documents in their initial retrieval.

Questions:

Where's the actual EQFE model?! Are you supposed to sum up the p{\_}{\{}RM{\}}(E) models or something? And is p{\_}{\{}RM{\}}(E) only for f{\_}{\{}RM{\}} features or for all features? Ugh.},
author = {Dalton, Jeffrey and Dietz, Laura and Allan, James},
booktitle = {SIGIR '14},
doi = {10.1145/2600428.2609628},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Dalton, Dietz, Allan - 2014 - Entity query feature expansion using knowledge base links.pdf:pdf},
isbn = {9781450322577},
keywords = {entities,information extraction,information retrieval,ontologies},
pages = {365--374},
title = {{Entity query feature expansion using knowledge base links}},
url = {http://dl.acm.org/citation.cfm?doid=2600428.2609628},
year = {2014}
}
@incollection{LaffertyJ.Zhai2003,
abstract = {We give a unified account of the probabilistic semantics underlaying the language modeling approach and the traditional probabilistic model for information retrieval, showing that the two approaches can be view as being equivalent probabilistically, since they are based on different factorizations of the same generative reference model. We also discuss how the two approaches lead to different retrieval frameworks in practice, since they involve component models that are estimated quite differently.},
annote = {Shows that the Robertson {\&} Sparck Jones model and the language model are two sides of the same probabilistic coin, i.e. that they are derived from the same probabilistic formalism. They are both generative (check this).

Discusses benefits of each factoring.

Draws connections to Maron and Kuhns, which they call query generation.},
author = {{Lafferty J., Zhai}, C},
booktitle = {Language Modeling for Information Retrieval},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Lafferty J., Zhai - 2003 - Probabilistic relevance models based on document and query generation.pdf:pdf},
pages = {1--10},
title = {{Probabilistic relevance models based on document and query generation}},
url = {http://en.scientificcommons.org/42653134},
year = {2003}
}
@article{Yang1999,
author = {Yang, Y},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Yang - 1999 - An Evaluation of Statistical Approaches to Text Categorization.pdf:pdf},
journal = {Information Retrieval},
keywords = {text-classification},
number = {1-2},
pages = {69--90},
title = {{An evaluation of statistical approaches to text categorization}},
volume = {1},
year = {1999}
}
@article{Cover2006,
author = {Cover, Thomas and Thomas, Joy},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Cover, Thomas - 2006 - Universal source coding.pdf:pdf},
journal = {Elements of Information Theory},
pages = {427--462},
title = {{Universal source coding}},
year = {2006}
}
@article{Dubin2004,
abstract = {Gerard Salton is often credited with developing the vector space model (VSM) for information retrieval (IR). Citations to Salton give the impression that the VSM must have been articulated as an IR model sometime between 1970 and 1975. However, the VSM as it is understood today evolved over a longer time period than is usually acknowledged, and an articulation of the model and its assumptions did not appear in print until several years after those assumptions had been criticized and alternative models proposed. An often cited overview paper titled “A Vector Space Model for Information Retrieval” (alleged to have been published in 1975) does not exist, and citations to it represent a confusion of two 1975 articles, neither of which were overviews of the VSM as a model of information retrieval. Until the late 1970s, Salton did not present vector spaces as models of IR generally but rather as models of specifi c computations. Citations to the phantom paper refl ect an apparently widely held misconception that the operational features and explanatory devices now associated with the VSM must have been introduced at the same time it was fi rst proposed as an IR model.},
author = {Dubin, David},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Dubin - 2004 - The Most Influential Paper Gerard Salton Never Wrote.pdf:pdf},
isbn = {0024-2594},
issn = {0024-2594},
journal = {Library Trends},
number = {4},
pages = {748--764},
title = {{The most influential paper Gerard Salton never wrote}},
volume = {52},
year = {2004}
}
@article{Borlund2003,
author = {Borlund, Pia},
doi = {http://dx.doi.org/10.1002/asi.10286},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Borlund - 2003 - The concept of relevance in information retrieval.pdf:pdf},
isbn = {1532-2882},
journal = {ASIS{\&}T},
number = {10},
pages = {913--925},
title = {{The concept of relevance in information retrieval}},
volume = {54},
year = {2003}
}
@inproceedings{Efron2012,
abstract = {Collections containing a large number of short documents are becoming increasingly common. As these collections grow in number and size, providing effective retrieval of brief texts presents a significant research problem. We propose a novel approach to improving information retrieval (IR) for short texts based on aggressive document expansion. Starting from the hypothesis that short documents tend to be about a single topic, we submit documents as pseudo-queries and analyze the results to learn about the documents themselves. Document expansion helps in this context because short documents yield little in the way of term frequency information. However, as we show, the proposed technique helps us model not only lexical properties, but also temporal properties of documents. We present experimental results using a corpus of microblog (Twitter) data and a corpus of metadata records from a federated digital library. With respect to established baselines, results of these experiments show that applying our proposed document expansion method yields significant improvements in effectiveness. Specifically, our method improves the lexical representation of documents and the ability to let time influence retrieval.},
annote = {This is almost identical to what I'm doing, but:

1. It's specific to the situation in which documents are short. Therefore, my approach adds the complexity of dealing with longer documents.

2. It does not use an external collection at all.

3. Relevance modeling tanks results. Gotta ask Miles about that bit.},
author = {Efron, Miles and Organisciak, Peter and Fenlon, Katrina},
booktitle = {SIGIR '12},
doi = {10.1145/2348283.2348405},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Efron, Organisciak, Fenlon - 2012 - Improving Retrieval of Short Texts Through Document Expansion.pdf:pdf},
isbn = {978-1-4503-1472-5},
issn = {1450314724},
keywords = {information retrieval,language models,microblogs,temporal ir,twitter,ument expansion},
pages = {911--920},
title = {{Improving retrieval of short texts through document expansion}},
url = {http://dx.doi.org/10.1145/2348283.2348405},
year = {2012}
}
@inproceedings{Cao2007,
abstract = {The paper is concerned with learning to rank, which is to construct a model or a function for ranking objects. Learning to rank is useful for document retrieval, collaborative filtering, and many other applications. Several methods for learning to rank have been proposed, which take object pairs as 'instances' in learning. We refer to them as the pairwise approach in this paper. Al-though the pairwise approach offers advantages, it ignores the fact that ranking is a prediction task on list of objects. The paper postulates that learn-ing to rank should adopt the listwise approach in which lists of objects are used as 'instances' in learning. The paper proposes a new probabilis-tic method for the approach. Specifically it in-troduces two probability models, respectively re-ferred to as permutation probability and top one probability, to define a listwise loss function for learning. Neural Network and Gradient Descent are then employed as model and algorithm in the learning method. Experimental results on infor-mation retrieval show that the proposed listwise approach performs better than the pairwise ap-proach. Microsoft technique report. A short version of this work is pub-lished in ICML2007.},
author = {Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and Li, Hang},
booktitle = {International conference on machine learning},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Cao et al. - 2007 - Learning to Rank From Pairwise Approach to Listwise Approach(2).pdf:pdf},
pages = {129--136},
title = {{Learning to rank: From pairwise approach to listwise approach}},
year = {2007}
}
@phdthesis{Fang2007,
author = {Fang, Hui},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Fang - 2007 - An Axiomatic Approach To Information Retrieval.pdf:pdf},
school = {University of Illinois at Urbana-Champaign},
title = {{An axiomatic approach To information retrieval}},
url = {https://www.ideals.illinois.edu/bitstream/handle/2142/11352/An Axiomatic Approach to Information Retrieval.pdf?sequence=2},
year = {2007}
}
@article{Deerwester1990,
abstract = {retrieval system, is to find some way to predict what terms ``really'' are implied by a query or apply to a document (ie, the ``latent '') on the},
author = {Deerwester, S and Dumais, S and Furnas, G and Landauer, T and Harshman, R},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Deerwester et al. - 1990 - Indexing by Latent Semantic Analysis.pdf:pdf},
isbn = {0002-8231},
journal = {JASIS},
number = {6},
pages = {391--407},
title = {{Indexing by latent semantic analysis}},
volume = {41},
year = {1990}
}
@inproceedings{Newman2010,
annote = {Basically they constructed a "gold standard" set of human annotations regarding the coherence of topic models. They then use a whole mess of different scoring methods based on Google, WordNet, and Wikipedia to produce scores that they theorize will correspond to topic coherence, as determined by the human annotators.

They find that inter-annotator agreement is very high, though they don't use kappa like they probably should.

They find that pointwise mutual information is the most consistent and that Wikipedia data is most useful. 

I would say that it's good PMI works well because most of their other scoring methods are very heuristic and ad-hoc. But PMI has a nice intuitive meaning: it's the mutual information (essentially measure of independence) of two specific outcomes (each pair of terms in the topic model).},
author = {Newman, David and Lau, Jey Han and Grieser, Karl and Baldwin, Timothy},
booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Newman et al. - 2010 - Automatic evaluation of topic coherence.pdf:pdf},
pages = {100--108},
title = {{Automatic evaluation of topic coherence}},
year = {2010}
}
@book{Melucci2011,
abstract = {Information retrieval is the science concerned with the effective and efficient retrieval of documents starting from their semantic content. It is employed to fulfill some information need from a large number of digital documents. Given the ever-growing amount of documents available and the heterogeneous data structures used for storage, information retrieval has recently faced and tackled novel applications.$\backslash$n$\backslash$nIn this book, Melucci and Baeza-Yates present a wide-spectrum illustration of recent research results in advanced areas related to information retrieval. Readers will find chapters on e.g. aggregated search, digital advertising, digital libraries, discovery of spam and opinions, information retrieval in context, multimedia resource discovery, quantum mechanics applied to information retrieval, scalability challenges in web search engines, and interactive information retrieval evaluation. All chapters are written by well-known researchers, are completely self-contained and comprehensive, and are complemented by an integrated bibliography and subject index.$\backslash$n$\backslash$nWith this selection, the editors provide the most up-to-date survey of topics usually not addressed in depth in traditional (text)books on information retrieval. The presentation is intended for a wide audience of people interested in information retrieval: undergraduate and graduate students, post-doctoral researchers, lecturers, and industrial researchers.},
author = {Melucci, Massimo and Baeza-Yates, Ricardo},
doi = {10.1007/978-3-642-20946-8},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Melucci, Baeza-Yates - 2011 - Advanced Topics in Information Retrieval.pdf:pdf},
isbn = {3642209467},
pages = {308},
title = {{Advanced topics in information retrieval}},
url = {http://books.google.com/books?id=yFD0YjXoiiYC{\&}pgis=1},
volume = {33},
year = {2011}
}
@inproceedings{Nasr2004,
author = {Nasr, Alexis and B{\'{e}}chet, Fr{\'{e}}d{\'{e}}ric and Volanschi, Alexandra},
booktitle = {Proceedings of the 20th international conference on Computational Linguistics - COLING '04},
doi = {10.3115/1220355.1220437},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Nasr, B{\'{e}}chet, Volanschi - 2004 - Tagging with hidden Markov models using ambiguous tags.pdf:pdf},
pages = {569},
title = {{Tagging with hidden Markov models using ambiguous tags}},
url = {http://portal.acm.org/citation.cfm?doid=1220355.1220437},
year = {2004}
}
@article{Harter1992,
abstract = {This article summarizes the theory of psychological relevance proposed by Dan Sperber and Deirdre Wilson (1988), to explicate the relevance of speech utterances to hearers in everyday conversation. The theory is then interpreted as the concept of relevance in information retrieval, and an extended example is presented. Im- plications of psychological relevance for research in information retrieval; evaluation of information retrieval systems; and the concepts of information, information need, and the information-seeking process are explored. Connections of the theory to ideas in bibliometrics are also suggested.},
annote = {A lot of similarity to my definition of relevance throughout.

Psychological relevance is explicitly in contrast to topical relevance.

Leaves questions about systems relevance.},
author = {Harter, Stephen P.},
doi = {10.1002/(SICI)1097-4571(199210)43:9<602::AID-ASI3>3.0.CO;2-Q},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Harter - 1992 - Psychological relevance and information science.pdf:pdf},
isbn = {0002-8231},
issn = {0002-8231},
journal = {JASIST},
number = {9},
pages = {602--615},
title = {{Psychological relevance and information science}},
url = {http://doi.wiley.com/10.1002/(SICI)1097-4571(199210)43:9{\%}3C602::AID-ASI3{\%}3E3.0.CO;2-Q},
volume = {43},
year = {1992}
}
@inproceedings{Metzler2007,
author = {Metzler, Donald and Croft, W Bruce and Croft, W Bruce},
booktitle = {SIGIR '07},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Metzler, Croft, Croft - 2007 - Latent Concept Expansion Using Markov Random Fields.pdf:pdf},
isbn = {9781595935977},
keywords = {information retrieval,markov random elds,query expansion},
pages = {311--318},
title = {{Latent concept expansion using Markov random fields}},
year = {2007}
}
@article{Salton1965,
author = {Salton, Gerard},
doi = {10.1002/asi.5090160308},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Salton - 1965 - The evaluation of automatic retrieval procedures— selected test results using the SMART system.pdf:pdf},
issn = {0096946X},
journal = {American Documentation},
number = {3},
pages = {209--222},
title = {{The evaluation of automatic retrieval procedures— selected test results using the SMART system}},
url = {http://doi.wiley.com/10.1002/asi.5090160308},
volume = {16},
year = {1965}
}
@inproceedings{Cleverdon1991,
author = {Cleverdon, Cyril},
booktitle = {SIGIR '91},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Cleverdon - 1991 - The Significance of the Cranfield Tests on Index Languages.pdf:pdf},
pages = {3--12},
title = {{The significance of the Cranfield tests on index languages}},
year = {1991}
}
@inproceedings{Joachims1998,
abstract = {This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning.},
author = {Joachims, Thorsten},
booktitle = {ECML '98},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Joachims - 1998 - Text Categorization with Support Vector Machines Learning with Many Relevant Features.pdf:pdf},
isbn = {3540644172},
pages = {137--142},
title = {{Text categorization with support vector machines: Learning with many relevant features}},
url = {http://www.springerlink.com/index/drhq581108850171.pdf},
year = {1998}
}
@article{Cooper1971,
abstract = {The concept of “relevance”, sometimes also called “pertinence” or “aboutness”, is central to the theory of information retrieval. Unfortunately, however, there is at present no consensus as to how this notion should be defined. The purpose of this paper is to propose and defend a definition of what it means to say that a piece of stored information is “relevant” to the information need of a retrieval system user. The suggested definition explicates relevance in terms of logical implication. For any yes-or-no question answering system which operates with one of the standard formalized languages, the definition provides a mathematically precise criterion of relevance. For other types of fact retrieval systems and reference retrieval systems, including all systems whose stored information is expressed in natural language, the definition is not mathematically precise but is nevertheless still helpful on a conceptual level.},
author = {Cooper, W.S.},
doi = {10.1016/0020-0271(71)90024-6},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Cooper - 1971 - A definition of relevance for information retrieval.pdf:pdf},
isbn = {0020-0271},
issn = {00200271},
journal = {Information Storage and Retrieval},
number = {1},
pages = {19--37},
title = {{A definition of relevance for information retrieval}},
url = {http://www.sciencedirect.com/science/article/pii/0020027171900246},
volume = {7},
year = {1971}
}
@article{Hjorland2010,
author = {Hjorland, Birger},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Hjorland - 2010 - The Foundation of the Concept of Relevance.pdf:pdf},
isbn = {9783848215430},
issn = {14923831},
journal = {JASIST},
number = {2},
pages = {217--237},
title = {{The Foundation of the concept of relevance}},
volume = {61},
year = {2010}
}
@article{Mizzaro1998,
abstract = {The aim of an information retrieval system is to find relevant documents, thus relevance is a (if not ‘the') central concept of information retrieval. Notwithstanding its importance, and the huge amount of research on this topic in the past, relevance is not yet a well understood concept, also because of inconsistently used terminology. In this paper, I try to clarify this issue, classifying the various kinds of relevance. I show that: (i) there are many kinds of relevance, not just one; (ii) these kinds can be classified in a formally defined four dimensional space, and (iii) such classification helps us to understand the nature of relevance and relevance judgement. Finally, the consequences of this classification on the design and evaluation of information retrieval systems are analysed.},
author = {Mizzaro, Stefano},
doi = {10.1016/S0953-5438(98)00012-5},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Mizzaro - 1998 - How many relevances in information retrieval.pdf:pdf},
isbn = {0953-5438},
issn = {09535438},
journal = {Interacting with Computers},
keywords = {design,information retrieval,system evaluation},
number = {3},
pages = {303--320},
title = {{How many relevances in information retrieval?}},
volume = {10},
year = {1998}
}
@article{Robertson1982,
abstract = {Two models of document retrieval are analysed - Maron and Kuhns' (Model 1) groups users together to compute a probability of relevance for a given document; and Robertson and Sparck Jones' (Model 2) groups documents together to compute a probability of relevance for a given user. A unified theory is presented detailing four models; higher-level Model 3 depicts users making a relevance judgment on a document as the interaction of two sets of events: individual user with group of documents, and group of users with individual document. A simplified system of unified theory includes the nature of indexing, query formulation, complementarity of Models 1 and 2, and implications for probability ranking principles.},
author = {Robertson, S E and Maron, M E and Cooper, W S},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Robertson, Maron, Cooper - 1982 - Probability of relevance a unification of two competing models for document retrieval.pdf:pdf},
journal = {Information Technology: Research and Development},
number = {November},
pages = {1--21},
title = {{Probability of relevance: a unification of two competing models for document retrieval}},
url = {http://www.soi.city.ac.uk/{~}ser/papers/Unified{\_}Model.pdf},
volume = {1},
year = {1982}
}
@article{Schamber1990,
abstract = {Although relevance judgments are fundamental to the design and evaluation of all information retrieval systems, information scientists have not reached a consensus in defining the central concept of relevance. In this paper we ask two questions: What is the meaning of relevance? and What role does relevance play in information behavior? We attempt to address these questions by reviewing literature over the last 30 years that presents various views of relevance as topical, user-oriented, multidimensional, cognitive, and dynamic. We then discuss traditional assumptions on which most research in the field has been based and begin building a case for an approach to the problem of definition based on alternative assumptions. The dynamic, situational approach we suggest views the user — regardless of system — as the central and active determinant of the dimensions of relevance. We believe that relevance is a multidimensional concept; that it is dependent on both internal (cognitive) and external (situational) factors; that it is based on a dynamic human judgment process; and that it is a complex but systematic and measurable phenomenon.},
author = {Schamber, Linda and Eisenberg, Michael B. and Nilan, Michael S.},
doi = {10.1016/0306-4573(90)90050-C},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Schamber, Eisenberg, Nilan - 1990 - A re-examination of relevance toward a dynamic, situational definition∗.pdf:pdf},
isbn = {0306-4573},
issn = {03064573},
journal = {Information Processing {\&} Management},
number = {6},
pages = {755--776},
title = {{A re-examination of relevance: toward a dynamic, situational definition}},
url = {http://www.sciencedirect.com/science/article/pii/030645739090050C},
volume = {26},
year = {1990}
}
@inproceedings{Robertson,
author = {Robertson, Stephen and Walker, S and Jones, S and Hancock-Beaulieu, MM and Gatford, M},
booktitle = {TREC},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Robertson et al. - Unknown - Okapi at TREC-3.pdf:pdf},
title = {{Okapi at TREC-3}},
year = {2000}
}
@inproceedings{Diaz2006,
abstract = {Information retrieval algorithms leverage various collection statistics to improve performance. Because these statistics are often computed on a relatively small evaluation corpus, we believe using larger, non-evaluation corpora should im- prove performance. Specifically, we advocate incorporating external corpora based on language modeling. We refer to this process as external expansion. When compared to tra- ditional pseudo-relevance feedback techniques, external ex- pansion is more stable across topics and up to 10{\%} more effective in terms of mean average precision. Our results show that using a high quality corpus that is comparable to the evaluation corpus can be as, if not more, effective than using the web. Our results also show that external expansion outperforms simulated relevance feedback. In addition, we propose amethod for predicting the extent to which external expansion will improve retrieval performance. Our newmea- sure demonstrates positive correlation with improvements in mean average precision.},
author = {Diaz, Fernando and Metzler, Donald},
booktitle = {SIGIR '06},
doi = {10.1145/1148170.1148200},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Diaz, Metzler - 2006 - Improving the estimation of relevance models using large external corpora(2).pdf:pdf;:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Diaz, Metzler - 2006 - Improving the estimation of relevance models using large external corpora.pdf:pdf},
isbn = {1595933697},
keywords = {anguage models,els,language mod,pseudo relevance feedback,relevance feedback,relevance models},
pages = {154--161},
title = {{Improving the estimation of relevance models using large external corpora}},
url = {http://portal.acm.org/citation.cfm?doid=1148170.1148200},
year = {2006}
}
@article{McCallum1998,
abstract = {Recent approaches to text classification have used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e.g. Larkey and Croft 1996; Koller and Sahami 1997). Others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. Lewis and Gale 1994; Mitchell 1997). This paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora. We find that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes—providing on average a 27{\%} reduction in error over the multi-variate Bernoulli model at any vocabulary size. Introduction},
annote = {Clarifies differences between multiple-Bernoulli naive Bayes and multinomial naive Bayes.

The naive Bayes assumption: "that the probability of each word occurring in a document is independent of the occurrence of other words in a document" (p. 2)

They find that the multinomial is almost always better than the multiple-Bernoulli. There are some advantages to multiple-Bernoulli, though, relating to non-term features and relaxing of the naive Bayes assumption (i.e. introduction of some dependencies).

Has a really good discussion of generative models -- use it.},
author = {McCallum, Andres and Nigam, Kamal},
doi = {10.1.1.46.1529},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/McCallum, Nigam - 1998 - A Comparison of Event Models for Naive Bayes Text Classification.pdf:pdf},
isbn = {0897915240},
issn = {0343-6993},
journal = {AAAI/ICML-98 Workshop on Learning for Text Categorization},
pages = {41--48},
title = {{A comparison of event models for naive bayes text classification}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.9324{\&}rep=rep1{\&}type=pdf},
year = {1998}
}
@article{Blei2010,
author = {Blei, David M. and Carin, Lawrence and Dunson, David},
doi = {10.1109/MSP.2010.938079},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Blei, Carin, Dunson - 2010 - Probabilistic Topic Models.pdf:pdf},
journal = {IEEE Signal Mag},
number = {6},
pages = {55--65},
title = {{Probabilistic topic models}},
volume = {27},
year = {2010}
}
@inproceedings{Berger1999,
abstract = {We propose a new probabilistic approach to information retrieval based upon the ideas and methods of statistical machine translation. The central ingredient in this approach is a statistical model of how a user might distill or "translate" a given document into a query. To assess the relevance of a document to a user's query, we$\backslash$nestimate the probability that the query would have been generated as a translation of the document, and factor in the user's general preferences in the form of a prior distribution over documents. We propose a simple, well motivated model of the document-to-query translation process, and describe an algorithm for learning the parameters of this model in an unsupervised manner from a collection of documents. As we show, one can view this approach as a generalization and justication of$\backslash$nthe "language modeling" strategy recently proposed by Ponte and Croft. In a series of experiments on {\{}TREC{\}} data, a simple translation-based retrieval system performs well in comparison to conventional retrieval techniques. This prototype system only begins to tap the full potential of translation-based retrieval.},
author = {Berger, Adam and Lafferty, John},
booktitle = {SIGIR '99},
doi = {10.1145/312624.312681},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Berger, Lafferty - 1999 - Information retrieval as statistical translation.pdf:pdf},
isbn = {1581130961},
issn = {09042512},
keywords = {4,903 words,and formulas,document-query models,em algorithm,excluding gures,hidden alignments,information retrieval,language models,source-channel model,statistical machine translation,tables,word count},
pages = {222--229},
title = {{Information retrieval as statistical translation}},
url = {http://portal.acm.org/citation.cfm?doid=312624.312681},
year = {1999}
}
@inproceedings{Lafferty2001,
abstract = {We present a framework for information retrieval that combines document models and query models using a probabilistic ranking function based on Bayesian decision theory. The framework suggests an operational retrieval model that extends recent developments in the language modeling approach to information retrieval. A language model for each document is estimated, as well as a language model for each query, and the retrieval problem is cast in terms of risk minimization. The query language model can be exploited to model user preferences, the context of a query, synonomy and word senses. While recent work has incorporated word translation models for this purpose, we introduce a new method using Markov chains defined on a set of documents to estimate the query models. The Markov chain method has connections to algorithms from link analysis and social networks. The new approach is evaluated on TREC collections and compared to the basic language modeling approach and vector space models together with query expansion using Rocchio. Significant improvements are obtained over standard query expansion methods for strong baseline TF-IDF systems, with the greatest improvements attained for short queries on Web data.},
author = {Lafferty, John and Zhai, Chengxiang},
booktitle = {SIGIR '01},
doi = {10.1145/383952.383970},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Lafferty, Zhai - 2001 - Document language models, query models, and risk minimization for information retrieval.pdf:pdf},
isbn = {1581133316},
issn = {01635840},
pages = {111--119},
title = {{Document language models, query models, and risk minimization for information retrieval}},
url = {http://portal.acm.org/citation.cfm?doid=383952.383970},
year = {2001}
}
@inproceedings{Cleverdon1967,
abstract = {The investigation dealt with the effect which different devices have on the performance of index languages. It appeared that the most important consideration was the specificity of the index terms; within the context of the conditions existing in this test, single‐word terms were more effective than concept terms or a controlled vocabulary.},
author = {Cleverdon, Cyril},
booktitle = {Aslib Proceedings},
doi = {10.1108/eb050097},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Cleverdon - 1967 - The Cranfield tests on index language devices.pdf:pdf},
issn = {0001-253X},
number = {6},
pages = {173--194},
title = {{The Cranfield tests on index language devices}},
url = {http://www.emeraldinsight.com/doi/abs/10.1108/eb050097},
volume = {19},
year = {1967}
}
@article{Voorhees2002,
abstract = {Voorhees EM. The philosophy of information retrieval evaluation. In: Peters C, Braschler M, Gonzalo J, Kluck M, editor. Evaluation of Cross-Language Information Retrieval Systems : Second Workshop of the Cross-Language Evaluation Forum CLEF 2001. Darmstadt, Germany ; 2001. pp. 355 –3370. (Lecture Notes in Computer Science).},
annote = {Most useful for:

1. Assumptions of the Cranfield paradigm
2. Response to Zobel's pooling recommendation},
author = {Voorhees, Ellen M.},
doi = {10.1007/3-540-45691-0_34},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Voorhees - 2002 - The philosophy of information retrieval evaluation.pdf:pdf},
isbn = {978-3-540-44042-0},
journal = {Evaluation of cross-language information retrieval systems},
pages = {143--170},
title = {{The philosophy of information retrieval evaluation}},
url = {http://www.springerlink.com/index/VNQ9W8Q6LBE5PLUE.pdf},
year = {2002}
}
@inproceedings{Cucerzan2007,
abstract = {This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results. It describes in detail the disambiguation paradigm employed and the information extraction process from Wikipedia. Through a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document, as well as the agreement among the category tags associated with the candidate entities, the implemented system shows high disambiguation accuracy on both news stories and Wikipedia articles.},
annote = {Doesn't use Wikipedia text at all, but does employ Wikipedia structure (linked entities, alternative titles, etc.)

Just entity linking, no retrieval component.},
author = {Cucerzan, Silviu},
booktitle = {EMNLP-CoNLL '07},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Cucerzan - 2007 - Large-Scale Named Entity Disambiguation Based on Wikipedia Data.pdf:pdf},
isbn = {9788890354175},
issn = {02120062},
number = {June},
pages = {708--716},
title = {{Large-scale named entity disambiguation based on Wikipedia data}},
url = {http://research.microsoft.com/pubs/68124/emnlp07.pdf},
year = {2007}
}
@inproceedings{Zhao2010,
author = {Zhao, Le and Callan, Jamie},
booktitle = {CIKM '10},
doi = {10.1145/1871437.1871474},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Zhao, Callan - 2010 - Term necessity prediction.pdf:pdf},
isbn = {9781450300995},
keywords = {ad hoc retrieval models,mismatch,necessity,term weighting},
pages = {259--268},
title = {{Term necessity prediction}},
url = {http://portal.acm.org/citation.cfm?doid=1871437.1871474},
year = {2010}
}
@article{Lin2010,
abstract = {This half-day tutorial introduces participants to data-intensive text processing with the MapReduce programming model 1, using the open-source Hadoop implementation. The focus will be on scalability and the tradeoffs associated with distributed processing of large datasets. Content will include general discussions about algorithm design, presentation of illustrative algorithms, case studies in HLT applications, as well as practical advice in writing Hadoop programs and running Hadoop clusters.},
author = {Lin, Jimmy and Dyer, Chris},
doi = {10.2200/S00274ED1V01Y201006HLT007},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Lin, Dyer - 2010 - Data-Intensive Text Processing with MapReduce.pdf:pdf},
isbn = {1608453421},
issn = {1947-4040},
journal = {Synthesis Lectures on Human Language Technologies},
number = {1},
pages = {1--177},
title = {{Data-intensive text processing with MapReduce}},
volume = {3},
year = {2010}
}
@article{Robertson1977,
author = {Robertson, Stephen},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Robertson - 1977 - The Probability Ranking Principle in IR.pdf:pdf},
journal = {Journal of Documentation},
number = {4},
pages = {294--304},
title = {{The probability ranking principle in IR}},
volume = {33},
year = {1977}
}
@article{Jones1972,
abstract = {The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing, in particular, that frequently-occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure.},
author = {Jones, Karen Sparck},
doi = {10.1108/eb026526},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Jones - 1972 - a Statistical Interpretation of Term Specificity and Its Application in Retrieval.pdf:pdf},
isbn = {0947568212},
issn = {0022-0418},
journal = {Journal of Documentation},
number = {1},
pages = {11--21},
title = {{A statistical interpretation of term specificity and its application in retrieval}},
volume = {28},
year = {1972}
}
@book{Agresti2007,
abstract = {Praise for the First Edition "This is a superb text from which to teach categorical data analysis, at a variety of levels. . . this book can be very highly recommended." Short Book Reviews "Of great interest to potential readers is the variety of fields that are represented in the examples: health care, financial, government, product marketing, and sports, to name a few." Journal of Quality Technology "Alan Agresti has written another brilliant account of the analysis of categorical data." The Statistician The use of statistical methods for categorical data is ever increasing in today's world. An Introduction to Categorical Data Analysis, Second Edition provides an applied introduction to the most important methods for analyzing categorical data. This new edition summarizes methods that have long played a prominent role in data analysis, such as chi-squared tests, and also places special emphasis on logistic regression and other modeling techniques for univariate and correlated multivariate categorical responses. This Second Edition features: Two new chapters on the methods for clustered data, with an emphasis on generalized estimating equations (GEE) and random effects models A unified perspective based on generalized linear models An emphasis on logistic regression modeling An appendix that demonstrates the use of SAS for all methods An entertaining historical perspective on the development of the methods Specialized methods for ordinal data, small samples, multicategory data, and matched pairs More than 100 analyses of real data sets and nearly 300 exercises Written in an applied, nontechnical style, the book illustrates methods using a wide variety of real data, including medical clinical trials, drug use by teenagers, basketball shooting, horseshoe crab mating, environmental opinions, correlates of happiness, and much more. An Introduction to Categorical Data Analysis, Second Edition is an invaluable tool for social, behavioral, and biomedical scientists, as well as researchers in public health, marketing, education, biological and agricultural sciences, and industrial quality control.},
author = {Agresti, Alan},
booktitle = {Statistics},
doi = {10.1002/0471249688},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Agresti - 2007 - An Introduction to Categorical Data Analysis.pdf:pdf},
isbn = {9780471226185},
keywords = {9780470114742},
pages = {xvii, 372 p.},
pmid = {14562513},
title = {{An introduction to categorical data analysis}},
url = {http://www.loc.gov/catdir/enhancements/fy0826/2006042138-b.html},
year = {2007}
}
@article{Kurland2011,
abstract = {Exploiting information induced from (query-specific)clus- tering of top-retrieved documents has long been proposed as means for improving precision at the very top ranks of the returned results. We present a novel language model approach to ranking query-specific clusters by the presumed percentage of relevant documents that they contain. While most previous cluster ranking approaches focus on the clus- ter as a whole, our model also exploits information induced from documents associated with the cluster. Our model substantially outperforms previous approaches for identify- ing clusters containing a high relevant-document percentage. Furthermore, using the model to produce document rank- ing yields precision-at-top-ranks performance that is con- sistently better than that of the initial ranking upon which clustering is performed; the performance also favorably com- pares with that of a state-of-the-art pseudo-feedback re- trieval method.},
author = {Kurland, Oren and Krikon, Eyal},
doi = {10.1613/jair.3327},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/live-3327-5741-jair.pdf:pdf},
isbn = {9781605581644},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {367--395},
title = {{The opposite of smoothing: A language model approach to ranking query-specific document clusters}},
volume = {41},
year = {2011}
}
@article{Singh2006,
abstract = {The EM algorithm is an iterative procedure for computing maximum-likelihood estimates or posterior modes in problems with incomplete data or problems that can be formulated as such (e.g., with latent structures).},
author = {Singh, Ashok K},
doi = {10.1198/tech.2006.s354},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Singh - 2006 - The EM Algorithm.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
number = {1},
pages = {148--148},
title = {{The EM Algorithm}},
url = {http://pubs.amstat.org/doi/abs/10.1198/tech.2006.s354},
volume = {48},
year = {2006}
}
@article{VanRijsbergen1986,
author = {van Rijsbergen, C.J.},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/van Rijsbergen - 1986 - A non-classical logic for information retrieval.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {The Computer Journal},
number = {6},
pages = {481--485},
title = {{A non-classical logic for information retrieval}},
volume = {29},
year = {1986}
}
@book{Agosti2008,
address = {Berlin},
editor = {Agosti, Maristella},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Agosti - 2008 - Information access through search engines and digital libraries.pdf:pdf},
isbn = {9783540751335},
publisher = {Springer},
title = {{Information access through search engines and digital libraries}},
url = {http://link.springer.com/content/pdf/10.1007/978-3-540-75134-2.pdf},
year = {2008}
}
@article{Cover2006a,
author = {Cover, Thomas M. and Thomas, Joy a.},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Cover, Thomas - 2006 - Entropy, Relative Entropy, and Mutual Information.pdf:pdf},
isbn = {0471062596},
issn = {0471200611},
journal = {Elements of Information Theory},
number = {x},
pages = {13--55},
title = {{Entropy, Relative Entropy, and Mutual Information}},
url = {http://onlinelibrary.wiley.com.ezproxy.lib.purdue.edu/book/10.1002/047174882X;jsessionid=CFDDA55DA201485F6A2092EC7A278955.f02t03},
year = {2006}
}
@inproceedings{Tao2006,
abstract = {Language model information retrieval depends on accurate estimation of document models. In this paper, we propose a document expansion technique to deal with the problem of insufficient sampling of documents. We construct a probabilistic neighborhood for each document, and expand the document with its neighborhood information. The expanded document provides a more accurate estimation of the document model, thus improves retrieval accuracy. Moreover, since document expansion and pseudo feedback exploit different corpus structures, they can be combined to further improve performance. The experiment results on several different data sets demonstrate the effectiveness of the proposed document expansion method.},
author = {Tao, Tao and Wang, Xuanhui and Mei, Qiaozhu and Zhai, ChengXiang},
booktitle = {NAACL '06},
doi = {10.3115/1220835.1220887},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/N06-1052.pdf:pdf;:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Tao et al. - 2006 - Language Model Information Retrieval with Document Expansion.pdf:pdf},
pages = {407--414},
title = {{Language model information retrieval with document expansion}},
url = {http://portal.acm.org/citation.cfm?doid=1220835.1220887},
year = {2006}
}
@inproceedings{Ponte1998,
author = {Ponte, J M and Croft, W B},
booktitle = {SIGIR '98},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Ponte, Croft - 1998 - A Language Modeling Approach to Information Retrieval.pdf:pdf},
pages = {275--281},
title = {{A language modeling approach to information retrieval}},
year = {1998}
}
@article{Belkin1992,
abstract = {Information filtering systems are designed for unstructured or semistructured data, as opposed to database applications, which use very structured data. The systems also deal primarily with textual information, but they may also entail images, voice, video or other data types that are part of multimedia information systems. Information filtering systems also involve a large amount of data and streams of incoming data, whether broadcast from a remote source or sent directly by other sources. Filtering is based on descriptions of individual or group information preferences, or profiles, that typically represent long-term interests. Filtering also implies removal of data from an incoming stream rather than finding data in the stream; users see only the data that is extracted. Models of information retrieval and filtering, and lessons for filtering from retrieval research are presented.},
author = {Belkin, Nj and Croft, Wb},
doi = {10.1145/138859.138861},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Belkin, Croft - 1992 - Information filtering and information retrieval two sides of the same coin.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {10},
pages = {1--10},
title = {{Information filtering and information retrieval: two sides of the same coin?}},
url = {http://dl.acm.org/citation.cfm?id=138861},
volume = {29},
year = {1992}
}
@article{Maron1960,
abstract = {This paper reports on a novel technique for literature indexing and searching in a mechanized library system. The notion of relevance is taken as the key concept in the theory of information retrieval and a comparative concept of relevance is explicated in terms of the theory of probability. The resulting technique called Probabilistic Indexing, allows a computing machine, given a request for information, to make a statistical inference and derive a number (called the relevance number) for each document, which is a measure of the probability that the document will satisfy the given request. The result of a search is an ordered list of those documents which satisfy the request ranked according to their probable relevance. The paper goes on to show that whereas in a conventional library system the cross-referencing (see and see also) is based solely on the semantical closeness between index terms, statistical measures of closeness between index terms can be defined and computed. Thus, given an arbitrary request consisting of one (or many) index term(s), a machine can elaborate on it to increase the probability of selecting relevant documents that would not otherwise have been selected. Finally, the paper suggests an interpretation of the whole library problem as one where the request is considered as a clue on the basis of which the library system makes a concatenated statistical inference in order to provide as an output an ordered list of those documents which most probably satisfy the information needs of the user.},
author = {Maron, M. E. and Kuhns, J. L.},
doi = {10.1145/321033.321035},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Maron, Kuhns - 1960 - On Relevance, Probabilistic Indexing and Information Retrieval.pdf:pdf},
isbn = {0004-5411},
issn = {00045411},
journal = {Journal of the ACM},
number = {3},
pages = {216--244},
title = {{On relevance, probabilistic indexing and information retrieval}},
volume = {7},
year = {1960}
}
@inproceedings{Mitra1998,
author = {Mitra, Mandar and Singhal, Amit and Buckley, Chris},
booktitle = {SIGIR '98},
doi = {10.1145/290941.290995},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Mitra, Singhal, Buckley - 1998 - Improving automatic query expansion.pdf:pdf},
isbn = {1581130155},
pages = {206--214},
title = {{Improving automatic query expansion}},
url = {http://portal.acm.org/citation.cfm?doid=290941.290995},
year = {1998}
}
@article{Sjolund2013,
author = {Sj{\"{o}}lund, Jens},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Sj{\"{o}}lund - 2013 - Gaussian channel.pdf:pdf},
number = {May},
pages = {1--26},
title = {{Gaussian channel}},
year = {2013}
}
@book{Lavrenko2009,
annote = {Chapter 2 is very useful for discussion of why dependency models don't work well in either the BIM or the LM approaches -- they show it formally in the BIM and discuss it informally in the LM.},
author = {Lavrenko, Victor},
booktitle = {The Information Retrieval Series},
editor = {Croft, Bruce W.},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Lavrenko - 2009 - A Generative Theory of Relevance.pdf:pdf},
publisher = {Springer},
title = {{A generative theory of relevance}},
year = {2009}
}
@inproceedings{Zhai2002,
abstract = {The optimal settings of retrieval parameters often depend on both the document collection and the query, and are usually found through empirical tuning. In this paper, we propose a family of two-stage language models for information retrieval that explicitly},
author = {Zhai, ChengXiang and Lafferty, John},
booktitle = {SIGIR '02},
doi = {10.1145/564376.564387},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Zhai, Lafferty - 2002 - Two-stage language models for information retrieval.pdf:pdf},
isbn = {1-58113-561-0},
keywords = {dirichlet prior,ing,interpolation,leave-one-,parameter estimation,risk minimization,two-stage language models,two-stage smooth-},
pages = {49--56},
title = {{Two-stage language models for information retrieval}},
url = {http://portal.acm.org/citation.cfm?id=564376.564387{\&}coll=DL{\&}dl=ACM{\&}CFID=326626001{\&}CFTOKEN=74961485{\%}5Cnpapers2://publication/doi/10.1145/564376.564387},
year = {2002}
}
@inproceedings{Nenkova2004,
abstract = {We present an empirically grounded method for evaluating content selection in summariza- tion. It incorporates the idea that no single best model summary for a collection of documents exists. Our method quantifies the relative im- portance of facts to be conveyed. We argue that it is reliable, predictive and diagnostic, thus im- proves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.},
author = {Nenkova, Ani and Passonneau, Rebecca},
booktitle = {HLT-NAACL},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Nenkova, Passonneau - 2004 - Evaluating content selection in summarization The pyramid method.pdf:pdf},
pages = {145--152},
title = {{Evaluating content selection in summarization: The pyramid method}},
url = {papers2://publication/uuid/DC675E84-0A45-48B7-A26C-F08B4B9398D3},
year = {2004}
}
@inproceedings{Bailey2008,
abstract = {We investigate to what extent people making relevance judgements for a reusable IR test collection are exchangeable. We consider three classes of judge: "gold standard" judges, who are topic originators and are experts in a particular information seeking task; "silver standard" judges, who are task experts but did not create topics; and "bronze standard" judges, who are those who did not define topics and are not experts in the task.},
address = {Singapore},
author = {Bailey, Peter and Craswell, Nick and Soboroff, Ian and Thomas, Paul and de Vries, Arjen P and Yilmaz, Emine},
booktitle = {SIGIR '08},
doi = {10.1145/1390334.1390447},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Bailey et al. - 2008 - Relevance assessment Are judges exchangeable and does it matter.pdf:pdf},
isbn = {9781605581644},
pages = {667--674},
title = {{Relevance assessment: Are judges exchangeable and does it matter}},
url = {http://dl.acm.org/citation.cfm?id=1390447},
year = {2008}
}
@article{Cover1991,
author = {Cover, Thomas M and Thomas, Joy a},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Cover, Thomas - 1991 - Channel Capacity.pdf:pdf},
isbn = {0471062596},
pages = {183--241},
title = {{Channel Capacity}},
year = {1991}
}
@book{Koller2011,
author = {Koller, Daphne},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Koller - 2011 - Probabilistic Graphical Models.pdf:pdf},
isbn = {9780262013192},
pages = {1--19},
title = {{Probabilistic graphical models}},
year = {2011}
}
@book{Carmel2010,
abstract = {Abstract Many information retrieval (IR) systems suffer from a radical variance in performance when responding to users' queries. Even for systems that succeed very well on average, the quality of results returned for some of the queries is poor. Thus, it is desirable that IR systems will be able to identify "difficult" queries so they can be handled properly. Understanding why some queries are inherently more difficult than others is essential for IR, and a good answer to this important question will help search engines to reduce the variance in performance, hence better servicing their customer needs. Estimating the query difficulty is an attempt to quantify the quality of search results retrieved for a query from a given collection of documents. This book discusses the reasons that cause search engines to fail for some of the queries, and then reviews recent approaches for estimating query difficulty in the IR field. It then describes a common methodology for evaluating the prediction quality of those estimators, and experiments with some of the predictors applied by various IR methods over several TREC benchmarks. Finally, it discusses potential applications that can utilize query difficulty estimators by handling each query individually and selectively, based upon its estimated difficulty. Table of Contents: Introduction - The Robustness Problem of Information Retrieval / Basic Concepts / Query Performance Prediction Methods / Pre-Retrieval Prediction Methods / Post-Retrieval Prediction Methods / Combining Predictors / A General Model for Query Difficulty / Applications of Query Difficulty Estimation / Summary and Conclusions},
author = {Carmel, David and Yom-Tov, Elad},
booktitle = {Synthesis Lectures on Information Concepts, Retrieval, and Services},
doi = {10.2200/S00235ED1V01Y201004ICR015},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Carmel, Yom-Tov - 2010 - Estimating the Query Difficulty for Information Retrieval(2).pdf:pdf},
isbn = {1947-945X},
issn = {1947-945X},
number = {1},
pages = {1--89},
title = {{Estimating the query difficulty for information retrieval}},
volume = {2},
year = {2010}
}
@article{Saracevic2007,
author = {Saracevic, Tefko},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Saracevic - 2007 - Relevance A Review of the Literature and a Framework for Thinking on the Notion in Information Science. Part III Beha.pdf:pdf},
isbn = {9783848215430},
issn = {14923831},
journal = {JASIST},
number = {13},
pages = {2126--2144},
title = {{Relevance: A review of the literature and a framework for thinking on the notion in information science. Part III: behavior and effects of relevance}},
volume = {58},
year = {2007}
}
@misc{Sherman2015,
abstract = {My field exam},
address = {Champaign, IL},
annote = {From Duplicate 2 (Q1 : Evaluation in Textual Analysis Methods - Sherman, Garrick)

Contents:
- Ground Truth
- Metrics
- Priorities
- System comparison},
author = {Sherman, Garrick},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Sherman - 2015 - Field Exam.pdf:pdf;:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley//Sherman - 2015 - Q1 Evaluation in Textual Analysis Methods.pdf:pdf;:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Sherman - 2015 - Q2 Relevance in Information Retrieval.pdf:pdf;:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Sherman - 2015 - Q3 Dealing with Vocabulary Mismatch in Information Retrieval.pdf:pdf;:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Sherman - 2015 - Q6 Feature Selection in Information Retrieval.pdf:pdf;:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Sherman - 2015 - Q7 Term (In)dependence in Text Processing Methods.pdf:pdf},
pages = {1--6},
title = {{Field exam}},
year = {2015}
}
@article{Sebastiani1998,
abstract = {The logical approach to information retrieval has recently been the object of active re- search. It is our contention that researchers have put a lot of effort in trying to address some difficult problems of IR within this framework,but little effort in checking that the resulting models satisfy those well-formedness criteria that,in the field of mathematical logic,are considered essential and conducive to effective modelling of a real-world phe- nomenon. The main motivation of this paper is not to propose a new logical model of IR,but to discuss some central issues in the application of logic to IR. The first issue we touch upon is the logical relationship we might want to enforce between formulae d,re- presenting a document,and n,representing an information need; we analyse the different implications of models based on truth, validity or logical consequentiality. The relationship between this issue and the issue of partiality vs. totality of information is subsequently analysed,in the context of a broader discussion of the role of denotational semantics in IR modelling. Finally,the relationship between the paradoxes of material implication and the (in)adequacy of classical logic for IR modelling purposes is discussed.},
author = {Sebastiani, Fabrizio},
doi = {10.1016/S0306-4573(97)00055-1},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Sebastiani - 1998 - On the role of logic in information retrieval.pdf:pdf},
issn = {03064573},
journal = {Information Processing {\&} Management},
number = {1},
pages = {1--18},
title = {{On the role of logic in information retrieval}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0306457397000551},
volume = {34},
year = {1998}
}
@article{Croft2015,
author = {Croft, W Bruce},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley//Croft, Metzler, Strohmann - 2010 - Search Engines Information Retrieval in Practice.pdf:pdf},
title = {{SEIRiP}},
year = {2015}
}
@article{Salton1968,
abstract = {Automatic indexing methods are evaluated and design criteria for modern informa- tion systems are derived.},
author = {Salton, Gerard and Lesk, M. E.},
doi = {10.1145/321439.321441},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Salton, Lesk - 1968 - Computer Evaluation of Indexing and Text Processing.pdf:pdf},
isbn = {0004-5411},
issn = {00045411},
journal = {Journal of the ACM},
number = {1},
pages = {8--36},
title = {{Computer evaluation of indexing and text processing}},
volume = {15},
year = {1968}
}
@unpublished{Smucker2005,
author = {Smucker, Mark D and Kulp, David and Allan, James},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Smucker - 2005 - Dirichlet Mixtures for Query Estimation in Information Retrieval.pdf:pdf;:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Smucker, Kulp, Allan - 2005 - Dirichlet Mixtures for Query Estimation in Information Retrieval.pdf:pdf},
institution = {University of Massachusetts Amherst},
title = {{Dirichlet mixtures for query estimation in information retrieval}},
year = {2005}
}
@article{Salomon1988,
author = {Salomon, D.},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Salomon - 1988 - Data Compression.pdf:pdf},
isbn = {0387406972},
number = {x},
pages = {103--158},
title = {{Data compression}},
url = {http://www.csa.com/partners/viewrecord.php?requester=gs{\&}collection=TRD{\&}recid=1892920CI},
year = {1988}
}
@article{Salton1975,
abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
author = {Salton, Gerard and Wong, A. and Yang, C. S.},
doi = {10.1145/361219.361220},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Salton, Wong, Yang - 1975 - A vector space model for automatic indexing.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {and phrases,automatic indexing,automatic information,content analysis,document,retrieval},
number = {11},
pages = {613--620},
pmid = {15142973},
title = {{A vector space model for automatic indexing}},
volume = {18},
year = {1975}
}
@article{Saracevic1975,
abstract = {The article introduces the special issue dedicated to "The Phi- losophy of Infonnation, Its Nature, and Future Developments." It outlines the origins of the information society and then briefly discusses the definition of the philosophy of information, the possi- bility of reconciling nature and technology, the infonnational turn as a fourth revolution (after Copernicus, Darwin, and Freud), and the metaphysics of the infosphere},
author = {Saracevic, Tefko},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Saracevic - 1975 - Relevance A Review of and a Framework for the Thinking on the Notion in Information Science.pdf:pdf},
isbn = {1111111111},
journal = {JASIS},
number = {6},
pages = {321--343},
title = {{Relevance: A review of and a framework for the thinking on the notion in information science}},
volume = {26},
year = {1975}
}
@book{Zhang2008,
abstract = {The amount of digitized information available on the Internet, in digital libraries, and other forms of information systems grows at an exponential rate, while becoming more complex and more dynamic. As a consequence, information organization, information retrieval and the presentation of retrieval results have become more and more difficult.Information visualization offers a unique method to reveal hidden patterns and contextual information in a visual presentation and allows users to seek information in an intuitive way. Jin Zhang provides a systematic explanation of the latest advancements in information retrieval visualization from both theoretical and practical perspectives. He reviews the main approaches and techniques available in the field, explains theoretical relationships between information retrieval and information visualization, and presents major information retrieval visualization algorithms and models. He then takes a detailed look at the theory and applications of information retrieval visualization for Internet traffic analysis, and Internet information searching and browsing. The author also addresses challenges such as ambiguity, metaphorical applications, and system evaluation in information retrieval visualization environments. Finally, he compares these information retrieval visualization models from the perspectives of visual spaces, semantic frameworks, projection algorithms, ambiguity, and information retrieval, and discusses important issues of information retrieval visualization and research directions for future exploration.Readers of this book will gain an in-depth understanding of the current state of information retrieval visualization. They will be introduced to existing problems for researchers and professionals, along with technical and theoretical findings and advances made by leading researchers. The book also provides practical details for the implementation of an information retrieval visualization system.},
author = {Zhang, Jin},
booktitle = {Seminar},
doi = {http://dl.acm.org/citation.cfm?id=1349807},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Zhang - 2008 - Visualization for information retrieval.pdf:pdf},
isbn = {9783540751472},
number = {[23]},
pages = {xvii, 292 p.},
pmid = {15004018},
title = {{Visualization for information retrieval}},
url = {http://www.loc.gov/catdir/toc/fy0805/2007937243.html},
volume = {23},
year = {2008}
}
@misc{Guerra2009,
abstract = {The integration of usable and flexible analysis support in modelling environments is a key success factor in Model-Driven Development. In this paradigm, models are the core asset from which code is automatically generated, and thus ensuring model correctness is a fundamental quality control activity. For this purpose, a common approach is to transform the system models into formal semantic domains for verification. However, if the analysis results are not shown in a proper way to the end-user (e.g. in terms of the original language) they may become useless. In this paper we present a novel DSVL called BaVeL that facilitates the flexible annotation of verification results obtained in semantic domains to different formats, including the context of the original language. BaVeL is used in combination with a consistency framework, providing support for all steps in a verification process: acquisition of additional input data, transformation of the system models into semantic domains, verification, and flexible annotation of analysis results. The approach has been validated analytically by the cognitive dimensions framework, and empirically by its implementation and application to several DSVLs. Here we present a case study of a notation in the area of Digital Libraries, where the analysis is performed by transformations into Petri nets and a process algebra. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Guerra, Esther and de Lara, Juan and Malizia, Alessio and D{\'{i}}az, Paloma},
booktitle = {Information and Software Technology},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {0402594v3},
isbn = {0950-5849},
issn = {09505849},
keywords = {Back-annotation,Consistency,Domain-specific visual languages,Formal methods,Model transformation,Modelling environments},
number = {4},
pages = {769--784},
primaryClass = {arXiv:cond-mat},
title = {{Supporting user-oriented analysis for multi-view domain-specific visual languages}},
volume = {51},
year = {2009}
}
@inproceedings{Saracevic1997,
abstract = {The paper is the acceptance address for the 1997 ACM SIGIR Gerard Salton Award for Excellence in Research. In the preamble, the approach of dealing with the broader context of information science when considering information retrieval (IR) is justified. The first part contains personal reflections of the author related to the major events and issues that formed his professional life and research agenda. The second, and major part, considers the broad aspects of information science as a field: origin, problems addressed, areas of study, structure, specialties, paradigm splits, and education problems. The third part discusses the limits of information science in terms of internal limits imposed by the activities in the field and external limits imposed by the very human nature of information processing and use. Throughout, issues related to users and use are transposed, as being of primary concern.},
author = {Saracevic, Tefko},
booktitle = {SIGIR '97},
doi = {10.1145/270886.270889},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Saracevic - 1997 - Users lost reflections on the past, future, and limits of information science.pdf:pdf},
isbn = {0-89791-836-3},
issn = {01635840},
pages = {16--27},
title = {{Users lost: reflections on the past, future, and limits of information science}},
year = {1997}
}
@inproceedings{Metzler2005,
abstract = {This paper develops a general, formal framework for modeling term dependencies via Markov random fields. The model allows for arbitrary text features to be incorporated as evidence. In particular, we make use of features based on occurrences of single terms, ordered phrases, and unordered phrases. We explore full independence, sequential dependence, and full dependence variants of the model. A novel approach is developed to train the model that directly maximizes the mean average precision rather than maximizing the likelihood of the training data. Ad hoc retrieval experiments are presented on several newswire and web collections, including the GOV2 collection used at the TREC 2004 Terabyte Track. The results show significant improvements are possible by modeling dependencies, especially on the larger web collections.},
author = {Metzler, Donald and Croft, W Bruce},
booktitle = {SIGIR '05},
doi = {10.1145/1076034.1076115},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Metzler, Croft - 2005 - A Markov random field model for term dependencies.pdf:pdf},
isbn = {1595930345},
keywords = {information retrieval,markov,phrases,term dependence},
pages = {472--479},
title = {{A Markov random field model for term dependencies}},
url = {http://portal.acm.org/citation.cfm?doid=1076034.1076115},
year = {2005}
}
@inproceedings{Voorhees1998,
abstract = {Test collectins have traditionally been used by information retrieval$\backslash$nresearchers to improve their retrieval strategies. To be viable as$\backslash$na laboratory tool, a collection must reliably rank different retrieval$\backslash$nvariants according to their true effectiveness. In particular, the$\backslash$nrelative effectiveness of two retrieval strategies should be insensitive$\backslash$nto modest changes in the relevant document set since individual relevance$\backslash$nassessments are known to vary widely. The test collections developed$\backslash$nin the TREC workshops have become the collections of choice in the$\backslash$nretrieval research community. To verify their reliability, NIST investigated$\backslash$nthe effect changes in the relevance assessments have on the evaluation$\backslash$nof retrieval results. Very high correlations were found among the$\backslash$nrankings of systems produced using different relevance judgement$\backslash$nsets. The high correlations indicate that the comparative evaluation$\backslash$nof retrieval performance is stable despite substantial differences$\backslash$nin relevance judgements, and thus reaffirm the use of the TREC collections$\backslash$nas laboratory tools.},
author = {Voorhees, Ellen M},
booktitle = {SIGIR '98},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Voorhees - 1998 - Variations in Relevance Judgements and the Measurement of Retrieval Effectiveness.pdf:pdf},
isbn = {0306-4573},
issn = {03064573},
keywords = {inf{\_}retrieval},
pages = {315--323},
title = {{Variations in relevance judgements and the measurement of retrieval effectiveness}},
year = {1998}
}
@book{Wasilewski2012,
author = {Wasilewski, Piotr},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Wasilewski - 2012 - The Modern Algebra of Information Retrieval.pdf:pdf},
isbn = {9783540776581},
keywords = {evaluation methodology,evance,information need,information retrieval,need,semantic evaluation,semantic information retrieval,semantic modeling of information,semantic rel-,semantic search engine,semantic valuation measure,semantic value of document},
pages = {107--120},
title = {{The modern algebra of information retrieval}},
year = {2012}
}
@article{Robertson2000,
author = {Robertson, Stephen},
doi = {10.1145/373593.373597},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Robertson - 2000 - Salton Award Lecture on theoretical argument in information retrieval.pdf:pdf},
isbn = {1581132263},
issn = {01635840},
journal = {ACM SIGIR Forum},
number = {1},
pages = {1--10},
title = {{Salton Award lecture on theoretical argument in information retrieval}},
volume = {34},
year = {2000}
}
@inproceedings{Zhai2004,
abstract = {Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and to then rank documents by the likelihood of the query according to the estimated language model. A central issue in language model estimation is smoothing, the problem of adjusting the maximum likelihood estimator to compensate for data sparseness. In this article, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections. Experimental results show that not only is the retrieval performance generally sensitive to the smoothing parameters, but also the sensitivity pattern is affected by the query type, with performance being more sensitive to smoothing for verbose queries than for keyword queries. Verbose queries also generally require more aggressive smoothing to achieve optimal performance. This suggests that smoothing plays two different role--to make the estimated document language model more accurate and to "explain" the noninformative words in the query. In order to decouple these two distinct roles of smoothing, we propose a two-stage smoothing strategy, which yields better sensitivity patterns and facilitates the setting of smoothing parameters automatically. We further propose methods for estimating the smoothing parameters automatically. Evaluation on five different databases and four types of queries indicates that the two-stage smoothing method with the proposed parameter estimation methods consistently gives retrieval performance that is close to--or better than--the best results achieved using a single smoothing method and exhaustive parameter search on the test data.},
annote = {Change ("improve") the MLE of language models AND
bring in uncommon query words

Connects P(qi|c) -- the collection language model -- to IDF (p 2)

Connects alpha{\_}d to document length normalization (p 5)},
author = {Zhai, Chengxiang and Lafferty, John},
booktitle = {SIGIR '01},
doi = {10.1145/984321.984322},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Zhai, Lafferty - 2001 - A study of smoothing methods for language models applied to information retrieval.pdf:pdf},
isbn = {1581133316},
issn = {10468188},
pmid = {1814564},
title = {{A study of smoothing methods for language models applied to information retrieval}},
year = {2001}
}
@article{Turtle1991,
abstract = {The use of inference networks to support document retrieval is introduced. A network-based retrieval model is described and compared to conventional probabilistic and Boolean models. The performance of a retrieval system based on the inference network model is evaluated and compared to performance with conventional retrieval models.},
author = {Turtle, Howard And W. Bruce Croft and Turtle, Howard And W. Bruce Croft},
doi = {10.1145/125187.125188},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Turtle, Turtle - 1991 - Evaluation of an Inference Retrieval Model.pdf:pdf},
issn = {10468188},
journal = {TOIS},
number = {3},
pages = {187--222},
title = {{Evaluation of an inference retrieval model}},
volume = {9},
year = {1991}
}
@unpublished{Raghavan2004,
author = {Raghavan, H and Allan, J and McCallum, A},
booktitle = {KDD '04},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Raghavan, Allan, McCallum - 2004 - An exploration of entity models, collective classification and relation description.pdf:pdf},
isbn = {1581138881},
pages = {1--10},
title = {{An exploration of entity models, collective classification and relation description}},
year = {2004}
}
@book{Crofta,
author = {Croft, W Bruce},
booktitle = {Machine Learning},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Croft - Unknown - Charting a New Course Natural Language Processing and Information Retrieval THE KLUWER INTERNATIONAL SERIES Series Ed.pdf:pdf},
isbn = {9781402033438},
title = {{Charting a new course : Natural language processing and information retrieval}}
}
@inproceedings{Yang1997,
abstract = {An abstract is not available.},
author = {Yang, Yiming and Pedersen, Jan O},
booktitle = {ICML '97},
doi = {10.1.1.32.9956},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Yang, Pedersen - 1997 - A Comparative Study on Feature Selection in Text Categorization.pdf:pdf},
isbn = {1-55860-486-3},
issn = {1367-4803},
pages = {412--420},
title = {{A comparative study on feature selection in text categorization}},
year = {1997}
}
@inproceedings{SparckJones1988,
abstract = {This paper is in two parts, following the suggestion that I first comment on my own past experience in information retrieval, and then present my views on the present and future.},
author = {{Sp{\"{a}}rck Jones}, Karen},
booktitle = {SIGIR '88},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Sp{\"{a}}rck Jones - 1988 - A look back and a look forward.pdf:pdf},
pages = {13--29},
title = {{A look back and a look forward}},
url = {http://dl.acm.org/citation.cfm?id=62438},
year = {1988}
}
@article{Salton1990,
abstract = {Briefly describes the principal relevance feedback methods that have been introduced over the years and evaluates the effectiveness of the methods in producing improved query formulations. Prescriptions are given for conducting text retrieval operations iteratively using relevance feedback.},
annote = {Review of Rocchio and probabilistic relevance feedback.

Rocchio is simple and intuitive.

Paints probabilistic model as deeply flawed -- shows how offset terms (basically, smoothing terms) are problematic.

Ultimately finds that probabilistic retrieval models perform worse than "simpler" vector models.

Interesting comparisons through with RM1 and RM3.

Finds that feedback works best with short queries that have unsatisfactory initial retrievals.

Interesting perspective since manual feedback is assumed -- we don't want to return the same documents the second time, because they've already been reviewed!},
author = {Salton, Gerard and Buckley, Chris},
doi = {10.1002/(SICI)1097-4571(199006)41:4<288::AID-ASI8>3.0.CO;2-H},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Salton, Buckley - 1990 - Improving retrieval performance by relevance feedback.pdf:pdf},
isbn = {1558604545},
issn = {00028231},
journal = {JASIS},
number = {4},
pages = {288--297},
title = {{Improving retrieval performance by relevance feedback}},
url = {http://doi.wiley.com/10.1002/(SICI)1097-4571(199006)41:4{\%}3C288::AID-ASI8{\%}3E3.0.CO;2-H},
volume = {41},
year = {1990}
}
@book{Spink2005,
abstract = {**New Directions in Cognitive Information Retrieval** presents an exciting new direction for research into cognitive oriented information retrieval (IR) research, a direction based on an analysis of the user's problem situation and cognitive behavior when using the IR system. This contrasts with the current dominant IR research paradigm which concentrates on improving IR system matching performance. The chapters describe the leading edge concepts and models of cognitive IR that explore the nexus between human cognition, information and the social conditions that drive humans to seek information using IR systems. Chapter topics include: Polyrepresentation, cognitive overlap and the boomerang effect, Multitasking while conducting the search, Knowledge Diagram Visualizations of the topic space to facilitate user assimilation of information, Task, relevance, selection state, knowledge need and knowledge behavior, search training built into the search, children's collaboration for school projects, and other cognitive perspectives on IR concepts and issues. This book is directly relevant to information scientists, librarians, social scientists and computer scientists interested in Human Computer Interaction (HCI) usability issues. Undergraduate and graduate students, academics.},
author = {Spink, Amanda and Cole, Charles},
booktitle = {The Information Retrieval Series},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Spink, Cole - 2005 - New Directions in Cognitive Information Retrieval Series.pdf:pdf},
isbn = {0792399269},
pages = {TOC},
title = {{New directions in cognitive information retrieval series}},
year = {2005}
}
@book{Zhai2009,
author = {Zhai, ChengXiang},
booktitle = {Synthesis Lectures on Human Language Technologies},
doi = {10.2200/S00158ED1V01Y200811HLT001},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Zhai - 2009 - Statistical Language Models for Information Retrieval.pdf:pdf},
isbn = {9781598295900},
issn = {1947-4040},
pages = {1--141},
title = {{Statistical language models for information retrieval}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00158ED1V01Y200811HLT001},
year = {2009}
}
@inproceedings{Adafre2007,
address = {Borovets, Bulgaria},
author = {Adafre, Sisay Fissaha and de Rijke, Maarten and Sang, Erik Tjong Kim},
booktitle = {RANLP '07},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Adafre, Rijke, Sang - 2007 - Entity retrieval.pdf:pdf},
keywords = {entity retrieval,language modeling,wikipedia},
pages = {5--11},
title = {{Entity retrieval}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.5983{\&}rep=rep1{\&}type=pdf},
year = {2007}
}
@inproceedings{Singhal1999,
abstract = {Advances in automatic speech recognition allow us to search large speech collections using traditional information retrieval methods. The problem of "aboutness " for documents--- is a document about a certain concept--- has been at the core of document indexing for the entire history of IR. This problem is more difficult for speech indexing since automatic speech transcriptions often contain mistakes. In this study we show that document expansion can be successfully used to alleviate the effect of transcription mistakes on speech retrieval. The loss of retrieval effectiveness due to automatic transcription errors can be reduced by document expansion from 15--27 {\%} relative to retrieval from human transcriptions to only about 7--13{\%}, even for automatic transcriptions with word error rates as high as 65{\%}. For good automatic transcriptions (25 {\%} word error rate), retrieval effectiveness with document expansion is indistinguishable from retrieval from human transcriptions. This makes speech retrieval from automatic transcriptions, even poor ones, competitive with retrieval from perfect transcriptions.},
author = {Singhal, Amit and Pereira, Fernando},
booktitle = {SIGIR '99},
doi = {10.1145/312624.312645},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/10.1.1.38.7363.pdf:pdf},
isbn = {1581130961},
pages = {34--41},
title = {{Document expansion for speech retrieval}},
url = {http://portal.acm.org/citation.cfm?doid=312624.312645},
year = {1999}
}
@inproceedings{Lv2010,
abstract = {Pseudo-relevance feedback is an effective technique for improving retrieval results. Traditional feedback algorithms use a whole feedback document as a unit to extract words for query expansion, which is not optimal as a document may cover several different topics and thus contain much irrelevant information. In this paper, we study how to effectively select from feedback documents those words that are focused on the query topic based on positions of terms in feedback documents. We propose a positional relevance model (PRM) to address this problem in a unified probabilistic way. The proposed PRM is an extension of the relevance model to exploit term positions and proximity so as to assign more weights to words closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic. We develop two methods to estimate PRM based on different sampling processes. Experiment results on two large retrieval datasets show that the proposed PRM is effective and robust for pseudo-relevance feedback, significantly outperforming the relevance model in both document-based feedback and passage-based feedback.},
author = {Lv, Yuanhua and Zhai, Chengxiang},
booktitle = {SIGIR '10},
doi = {10.1145/1835449.1835546},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Lv, Zhai - 2010 - Positional Relevance Model for Pseudo-Relevance Feedback.pdf:pdf},
isbn = {9781450301534},
keywords = {passage-based feedback,po-,positional relevance model,proximity,pseudo-relevance feedback,sitional language model},
pages = {579--586},
title = {{Positional relevance model for pseudo-relevance feedback}},
year = {2010}
}
@inproceedings{Buckley2004,
abstract = {This paper examines whether the Cranfield evaluation methodology is$\backslash$nrobust to gross violations of the completeness assumption (i.e.,$\backslash$nthe assumption that all relevant documents within a test collection$\backslash$nhave been identified and are present in the collection). We show$\backslash$nthat current evaluation measures are not robust to substantially$\backslash$nincomplete relevance judgments. A new measure is introduced that$\backslash$nis both highly correlated with existing measures when complete judgments$\backslash$nare available and more robust to incomplete judgment sets. This finding$\backslash$nsuggests that substantially larger or dynamic test collections built$\backslash$nusing current pooling practices should be viable laboratory tools,$\backslash$ndespite the fact that the relevance information will be incomplete$\backslash$nand imperfect.},
author = {Buckley, Chris and Voorhees, Ellen M},
booktitle = {SIGIR '04},
doi = {10.1145/1008992.1009000},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Buckley, Voorhees - 2004 - Retrieval evaluation with incomplete information.pdf:pdf},
isbn = {1-58113-881-4},
keywords = {cranfield,incomplete judgments},
pages = {25--32},
title = {{Retrieval evaluation with incomplete information}},
url = {http://doi.acm.org/10.1145/1008992.1009000},
year = {2004}
}
@article{Woodward2002,
author = {Woodward, P M},
doi = {10.1088/0508-3443/4/9/514},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Woodward - 2002 - Information theory(2).pdf:pdf},
issn = {0508-3443},
journal = {British Journal of Applied Physics},
number = {9},
pages = {288--288},
title = {{Information theory}},
volume = {4},
year = {2002}
}
@article{Harter1996,
author = {Harter, Stephen P},
doi = {http://dx.doi.org/10.1002/(SICI)1097-4571(199601)47:1<37::AID-ASI4>3.3.CO;2-I},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Harter - 1996 - Variations in relevance assessments and the measurement of retrieval effectiveness.pdf:pdf},
issn = {0002-8231},
journal = {JASIS},
number = {1},
pages = {37--49},
title = {{Variations in relevance assessments and the measurement of retrieval effectiveness}},
volume = {47},
year = {1996}
}
@article{Salton1988,
abstract = {The experimental evidence accumulated over the past 20 years indicates that text indexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective termweighting systems. This article summarizes the insights gained in automatic term weighting, and provides baseline single-term-indexing models with which other more elaborate content analysis procedures can be compared.},
annote = {Tries to link TF-IDF with "other theoretically attractive retrieval models" (top of page 5).

The authors address the lack of theoretical rigor to term weighting (see above note) but then go on to test various combinations of arbitrary term weights (see Table 1).},
archivePrefix = {arXiv},
arxivId = {115},
author = {Salton, Gerard and Buckley, Christopher},
doi = {10.1016/0306-4573(88)90021-0},
eprint = {115},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Salton, Buckley - 1988 - Term-weighting approaches in automatic text retrieval.pdf:pdf},
isbn = {1558604545},
issn = {03064573},
journal = {Information Processing {\&} Management},
number = {5},
pages = {513--523},
title = {{Term-weighting approaches in automatic text retrieval}},
url = {http://www.cs.odu.edu/{~}jbollen/IR04/readings/article1-29-03.pdf},
volume = {24},
year = {1988}
}
@inproceedings{Raviv2012,
author = {Raviv, Hadas and Kurland, Oren and Carmel, David},
booktitle = {SIGIR '16},
doi = {10.1145/2911451.2911508},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/p65-raviv.pdf:pdf},
isbn = {9781450340694},
keywords = {document retrieval,entity-based language models},
pages = {65--74},
title = {{Document retrieval using entity-based language models}},
year = {2016}
}
@book{Ingwersen2005,
author = {Ingwersen, Peter and J{\"{a}}rvelin, Kalervo},
booktitle = {The Kluwer International Series on Information Retrieval},
editor = {Croft, W. Bruce},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Ingwersen, J{\"{a}}rvelin - 2005 - The Turn Integration of Information Seeking and Retrieval in Context.pdf:pdf},
isbn = {9781402038501},
publisher = {Springer},
title = {{The turn: Integration of information seeking and retrieval in context}},
year = {2005}
}
@inproceedings{Lavrenko2002,
abstract = {We propose a formal model of Cross-Language Information Retrieval that does not rely on either query translation or document translation. Our approach leverages recent advances in language modeling to directly estimate an accurate topic model in the target language, starting with a query in the source language. The model integrates popular techniques of disambiguation and query expansion in a unified formal framework. We describe how the topic model can be estimated with either a parallel corpus or a dictionary. We test the framework by constructing Chinese topic models from English queries and using them in the CLIR task of TREC9. The model achieves performance around 95{\%} of the strong mono-lingual baseline in terms of average precision. In initial precision, our model outperforms the mono-lingual baseline by 20{\%}. The main contribution of this work is the unified formal model which integrates techniques that are essential for effective Cross-Language Retrieval.},
author = {Lavrenko, Victor and Choquette, Martin and Croft, W. Bruce},
booktitle = {SIGIR '02},
doi = {10.1145/564376.564408},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Lavrenko, Choquette, Croft - 2002 - Cross-lingual relevance models.pdf:pdf},
isbn = {1-58113-561-0},
issn = {01635840},
keywords = {cross-language information retrieval,language models},
pages = {175--182},
title = {{Cross-lingual relevance models}},
url = {http://portal.acm.org/citation.cfm?doid=564376.564408{\%}5Cnhttp://portal.acm.org/citation.cfm?id=564408},
year = {2002}
}
@inproceedings{Smucker2007,
author = {Smucker, Mark D and Allan, James and Carterette, Ben},
booktitle = {CIKM '07},
doi = {10.1145/1321440.1321528},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Smucker, Allan, Carterette - 2007 - A Comparison of Statistical Significance Tests for Information Retrieval Evaluation.pdf:pdf},
isbn = {9781595938039},
keywords = {bootstrap,dent,hypothesis test,permutation,randomization,s t-test,sign,statistical signi cance,stu-,wilcoxon},
pages = {623--632},
title = {{A comparison of statistical significance tests for information retrieval evaluation}},
year = {2007}
}
@inproceedings{Dalton2013,
abstract = {Entity Linking is the task of mapping a string in a document to its entity in a knowledge base. One of the crucial tasks is to identify disambiguating context; joint assignment models leverage the relationships within the knowledge base. We demonstrate how joint assignment models can be approximated with information retrieval. We introduce the neighborhood relevance model which uses relevance feedback techniques to identify the salience of entity context using cross-document evidence. We show that this model is more effective than local document models for ranking KB entities. Experiments on the TAC KBP entity linking task demonstrate that our model is the best performing system for strings that are linkable to the knowledge base.},
author = {Dalton, Jeffrey and Dietz, Laura},
booktitle = {OAIR '13},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Dalton, Dietz - 2013 - A neighborhood relevance model for entity linking.pdf:pdf},
isbn = {9782905450098},
pages = {149--156},
title = {{A neighborhood relevance model for entity linking}},
url = {http://dl.acm.org/citation.cfm?id=2491781},
year = {2013}
}
@article{Zhai2004b,
abstract = {Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and to then rank documents by the likelihood of the query according to the estimated language model. A central issue in language model estimation is smoothing, the problem of adjusting the maximum likelihood estimator to compensate for data sparseness. In this article, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections. Experimental results show that not only is the retrieval performance generally sensitive to the smoothing parameters, but also the sensitivity pattern is affected by the query type, with performance being more sensitive to smoothing for verbose queries than for keyword queries. Verbose queries also generally require more aggressive smoothing to achieve optimal performance. This suggests that smoothing plays two different role--to make the estimated document language model more accurate and to "explain" the noninformative words in the query. In order to decouple these two distinct roles of smoothing, we propose a two-stage smoothing strategy, which yields better sensitivity patterns and facilitates the setting of smoothing parameters automatically. We further propose methods for estimating the smoothing parameters automatically. Evaluation on five different databases and four types of queries indicates that the two-stage smoothing method with the proposed parameter estimation methods consistently gives retrieval performance that is close to--or better than--the best results achieved using a single smoothing method and exhaustive parameter search on the test data.},
author = {Zhai, Chengxiang and Lafferty, John},
doi = {10.1145/984321.984322},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Zhai, Lafferty - 2004 - A study of smoothing methods for language models applied to information retrieval(2).pdf:pdf},
isbn = {1581133316},
issn = {10468188},
journal = {TOIS},
number = {2},
pages = {179--214},
pmid = {1814564},
title = {{A study of smoothing methods for language models applied to information retrieval}},
volume = {22},
year = {2004}
}
@article{Gamal2011,
abstract = {This comprehensive treatment of network information theory and its applications provides the first unified coverage of both classical and recent results. With an approach that balances the introduction of new models and new coding techniques, readers are guided through Shannon's point-to-point information theory, single-hop networks, multihop networks, and extensions to distributed computing, secrecy, wireless communication, and networking. Elementary mathematical tools and techniques are used throughout, requiring only basic knowledge of probability, whilst unified proofs of coding theorems are based on a few simple lemmas, making the text accessible to newcomers. Key topics covered include successive cancellation and superposition coding, MIMO wireless communication, network coding, and cooperative relaying. Also covered are feedback and interactive communication, capacity approximations and scaling laws, and asynchronous and random access channels. This book is ideal for use in the classroom, for self-study, and as a reference for researchers and engineers in industry and academia.},
author = {Gamal, Abbas El and Kim, Young-Han and {El Gamal}, Abbas},
doi = {10.1017/CBO9781139030687},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Gamal, Kim, El Gamal - 2011 - Network information theory.pdf:pdf},
isbn = {9781107008731},
pages = {509--611},
title = {{Network information theory}},
url = {http://books.google.com/books?hl=pt-BR{\&}lr={\&}id=l31D4DU7jykC{\&}pgis=1},
year = {2011}
}
@inproceedings{Lv2009,
abstract = {Although many variants of language models have been proposed for information retrieval, there are two related retrieval heuristics remaining external to the language mod- eling approach: (1) proximity heuristic which rewards a doc- ument where the matched query terms occur close to each other; (2) passage retrieval which scores a document mainly based on the best matching passage. Existing studies have only attempted to use a standard language model as ablack box to implement these heuristics, making it hard to optimize the combination parameters. In this paper, we propose a novel positional language model (PLM) which implements both heuristics in a unified language model. The key idea is to define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of soft passage retrieval. We propose and study several representative density functions and several different PLM-based document ranking strategies. Experiment results on standard TREC test collections show that the PLM is effective for passage retrieval and performs better than a state-of-the-art proximity-based retrieval model.},
author = {Lv, Yuanhua and Zhai, ChengXiang},
booktitle = {SIGIR '09},
doi = {10.1145/1571941.1571994},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Lv, Zhai - 2009 - Positional language models for information retrieval.pdf:pdf},
isbn = {9781605584836},
keywords = {passage retrieval,positional language models,proximity},
pages = {299},
title = {{Positional language models for information retrieval}},
url = {http://portal.acm.org/citation.cfm?doid=1571941.1571994},
year = {2009}
}
@article{Jones2007,
abstract = {Statistical approaches to document indexing and retrieval date back to the beginning of automation. This paper considers early ideas, how they developed, their status now, and the challenges to be tackled in the future},
author = {Jones, K. S.},
doi = {10.1109/ICCTA.2007.122},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Jones - 2007 - Statistics and retrieval Past and future.pdf:pdf},
isbn = {0769527701},
journal = {Proceedings - International Conference on Computing: Theory and Applications, ICCTA 2007},
number = {1945},
pages = {396--404},
title = {{Statistics and retrieval: Past and future}},
year = {2007}
}
@article{Metzler2004,
annote = {A good review of the inference network retrieval model.

Based on Bayesian networks, so probabilistically formal model. Allows for structured queries.

Information needs depend on queries, which combine beliefs about representations (query terms or phrases). Ultimately we are asking for the probability of the presence of each of these, i.e. the probability of the presence of a representation node. 

Historically uses heuristic tf-idf without any formal interpretation. They adapt it to use Jelinek-Mercer smoothed language modeling and show that under certain conditions the query likelihood model is achievable within the inference network model.},
author = {Metzler, Donald and Croft, W. Bruce},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Metzler, Croft - 2004 - Combining the Language Model and Inference Network Approaches to Retrieval.pdf:pdf},
journal = {Information Processing and Management},
title = {{Combining the language model and inference network approaches to retrieval}},
year = {2004}
}
@article{Jarvelin2002,
author = {J{\"{a}}rvelin, K and Kek{\"{a}}l{\"{a}}inen, J},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/J{\"{a}}rvelin, Kek{\"{a}}l{\"{a}}inen - 2002 - Cumulated gain-based evaluation of IR techniques.pdf:pdf},
journal = {TOIS},
number = {4},
pages = {422--446},
title = {{Cumulated gain-based evaluation of IR techniques}},
url = {http://dl.acm.org/citation.cfm?id=582418},
volume = {20},
year = {2002}
}
@unpublished{Smucker2007,
abstract = {In the language modeling approach to information retrieval, Dirichlet prior smoothing frequently outperforms Jelinek-Mercer smoothing. Both Dirichlet prior and Jelinek-Mercer are forms of linear interpolated smooth- ing. The only difference between them is that Dirichlet prior determines the amount of smoothing based on a document's length. Theory suggests that Dirichlet prior's advantage should be the result of better document model estimation, for Dirichlet prior sensibly smooths longer documents less. In contrast, our hypothesis was that Dirichlet prior's performance ad- vantage comes primarily from a penalization of shorter documents' scores. We conducted two experiments to test our hypothesis. In our first experiment, when we transformed the test collections to have a uniform prob- ability of relevance given document length, P(Rel|Len), Dirichlet prior's performance advantage disappeared. If Dirichlet prior's advantage came from better estimation, it should have retained that advantage even with a uniform P(Rel|Len). In our second experiment, we gave the known P(Rel|Len) as a document prior to the retrieval method. With the doc- ument prior, Jelinek-Mercer's performance increased to match Dirichlet prior and Dirichlet prior showed some degradation in performance. These results confirm our hypothesis. While better estimation was formerly a plausible explanation of Dirichlet prior's performance advantage, we now know that Dirichlet prior smoothing's advantage appears to come from its penalization of shorter documents.},
author = {Smucker, Mark D and Allan, James},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Allan - 2007 - An Investigation of Dirichlet Prior Smoothing ' s Performance Advantage.pdf:pdf;:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Smucker, Allan - 2007 - An Investigation of Dirichlet Prior Smoothing's Performance Advantage.pdf:pdf},
institution = {University of Massachusetts Amherst},
keywords = {dirichlet prior,document prior,eling,jelinek-mercer,language mod-,smoothing},
pages = {1--34},
title = {{An investigation of Dirichlet prior smoothing's performance advantage}},
year = {2007}
}
@inproceedings{Zobel1998,
abstract = {Two stages in measurement of techniques for information retrieval are gathering of documents for relevance assessment and use of the assessments to numerically evaluate effectiveness. We consider both of these stages in the context of the TREC experiments, to determine whether they lead to measurements that are trustworthy and fair. Our detailed empirical investigation of the TREC results shows that the measured relative performance of systems appears to be reliable, but that recall is overestimated: it is likely that many relevant documents have not been found. We propose a new pooling strategy that can significantly in- crease the number of relevant documents found for given effort, without compromising fairness.},
annote = {Investigate the validity of TREC experimentation.

Find that statistical significance testing is good, and they argue for Wilcoxon test over t or ANOVA.

Notion of reinforcement, where similar systems look better than those that are dissimilar because they end up receiving similar documents and contribute to the pool more, leading to better looking results (which may be misleading).

Find that recall is consistently underestimated. Their analysis shows that up to 50{\%} of relevant documents have not be discovered by pooling, so measures of recall are poor estimates.

Most important, propose a new variable pooling method by which one uses regression over a smaller initial pool to predict how many new relevance judgments will be found by increasing the pool depth by another 10 documents per system. If the cost-benefit is okay, then extend the pool by 10; otherwise, don't. They use existing judgments to show that this would significantly increase the number of relevant documents discovered for equivalent amounts of work.},
author = {Zobel, Justin},
booktitle = {SIGIR '98},
doi = {10.1145/290941.291014},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Zobel - 1998 - How Reliable are the Results of Large-scale Information Retrieval Experiments.pdf:pdf},
isbn = {1581130155},
issn = {01635840 (ISSN)},
pages = {307--314},
title = {{How reliable are the results of large-scale information retrieval experiments?}},
year = {1998}
}
@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Blei, Ng, Jordan - 2003 - Latent Dirichlet Allocation.pdf:pdf},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {lda,topic model},
number = {4-5},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet allocation}},
url = {http://www.cs.princeton.edu/{~}blei/lda-c/{\%}5Cnpapers2://publication/doi/10.1162/jmlr.2003.3.4-5.993{\%}5Cnpapers2://publication/uuid/4001D0D9-4F9C-4D8F-AE49-46ED6A224F4A{\%}5Cnpapers2://publication/uuid/7D10D5DA-B421-4D94-A3ED-028107B7F9B6{\%}5Cnhttp://www.crossref.org/jmlr},
volume = {3},
year = {2003}
}
@article{Mattar2012,
author = {Mattar, Marwan and {Rudary, Matthew Weisstein}, Eric},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Mattar, Rudary, Matthew Weisstein - 2012 - Differential Entropy.pdf:pdf},
journal = {MathWorld--A Wolfram Web Resource},
number = {X},
pages = {243--259},
title = {{Differential entropy}},
url = {http://mathworld.wolfram.com/DifferentialEntropy.html},
year = {2012}
}
@inproceedings{Lavrenko2001,
abstract = {We explore the relation between classical probabilistic models of information retrieval and the emerging language modeling approaches. It has long been recognized that the primary obstacle to effective performance of classical models is the need to estimate a relevance model: probabilities of words in the relevant class. We propose a novel technique for estimating these probabilities using the query alone. We demonstrate that our technique can produce highly accu- rate relevance models, addressing important notions of synonymy and polysemy. Our experiments show relevance models outper- forming baseline language modeling systems on TREC retrieval and TDT tracking tasks. The main contribution of this work is an effective formal method for estimating a relevance model with no training data.},
author = {Lavrenko, Victor and Croft, W. Bruce},
booktitle = {SIGIR '01},
doi = {10.1145/383952.383972},
file = {:home/garrick/Documents/School Work/Grad School/GSLIS/General/Mendeley/Lavrenko, Croft - 2001 - Relevance based language models.pdf:pdf},
isbn = {1581133316},
issn = {1581133316},
pages = {120--127},
title = {{Relevance based language models}},
url = {http://portal.acm.org/citation.cfm?doid=383952.383972},
year = {2001}
}
